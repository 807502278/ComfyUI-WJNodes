from io import BytesIO
from copy import copy
import os
from math import gcd
from functools import reduce

import requests
from tqdm import tqdm
# from xml.dom import minidom

import numpy as np
from PIL import Image, ImageOps, ImageSequence, ImageFilter
import torch
import torch.nn.functional as F

import folder_paths
import node_helpers
from ..moduel.image_utils import device_list, device_default, clean_data
from ..moduel.custom_class import any
from ..moduel.str_edit import is_safe_eval


# ------------------image load/save nodes--------------------
CATEGORY_NAME = "WJNode/ImageFile"

class Load_Image_From_Path:
    """
    æŒ‰è·¯å¾„åŠ è½½å›¾ç‰‡
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required":
            {
                "PathFileName": ("STRING", {"default": ""})
            }
        }
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("image1", "mask1")
    FUNCTION = "load_image"

    def load_image(self, PathFileName):
        # Removes any quotes from Explorer
        image_path = PathFileName.replace('"', "")
        i = None
        if image_path.startswith("http"):
            response = requests.get(image_path)
            i = Image.open(BytesIO(response.content)).convert("RGB")
        else:
            i = Image.open(image_path)
        i = ImageOps.exif_transpose(i)
        image = i.convert("RGB")
        image = np.array(image).astype(np.float32) / 255.0
        image = torch.from_numpy(image)[None,]
        if 'A' in i.getbands():
            mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0
            mask = 1. - torch.from_numpy(mask)
        else:
            mask = torch.zeros((64, 64), dtype=torch.float32,
                               device=device_default)
        return (image, mask)

class Load_Image_Adv:
    DESCRIPTION = """
        A load - image node with image - path output and flipped mask.
        Support jpg, png, jpeg, webp, tiff, bmp, gif, ico, svg format
        Can obtain images inside subfolders and individual images within the input directory.

        å¸¦å›¾ç‰‡è·¯å¾„è¾“å‡ºå’Œç¿»è½¬é®ç½©çš„åŠ è½½å›¾ç‰‡èŠ‚ç‚¹
        é»˜è®¤æ‰«æ jpg,png,jpeg,webp,tiff,bmp,gif,ico,svg æ ¼å¼
        å¯è·å–inputå†…å­æ–‡ä»¶å¤¹å†…çš„å›¾åƒå’Œå•ä¸ªå›¾åƒ
        
    """
    def __init__(self):
        self.input_dir = folder_paths.get_input_directory()
        self.files = []
        self.allowed_extensions = ['.jpg', '.png', '.jpeg', '.webp', '.tiff', '.bmp', '.gif', '.ico', '.svg']

    @classmethod
    def INPUT_TYPES(cls):
        instance = cls()
        instance.traverse_directory(instance.input_dir)
        return {
            "required": {
                "image": (sorted(instance.files), 
                          {"image_upload": True}),
                "invert_mask": ("BOOLEAN", {"default": False})
            },
        }

    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE", "MASK", "STRING")
    RETURN_NAMES = ("image", "mask", "path")
    FUNCTION = "load_image"

    def load_image(self, image, invert_mask):
        """
        def to_tensor(image_path):
            from PIL import Image
            image = Image.open(image_path)
            img_np = np.array(image).transpose(2, 0, 1)
            tensor = torch.from_numpy(img_np).float() / 255.0
            return tensor.repeat(1, 1, 1, 1)
        # ç®€å•çš„åŠ è½½å›¾ç‰‡åˆ°torchå¼ é‡
        """

        image_path = folder_paths.get_annotated_filepath(image)
        img = node_helpers.pillow(Image.open, image_path)

        output_images = []
        output_masks = []
        w, h = None, None

        excluded_formats = ['MPO']

        for i in ImageSequence.Iterator(img):
            i = node_helpers.pillow(ImageOps.exif_transpose, i)

            if i.mode == 'I':
                i = i.point(lambda i: i * (1 / 255))
            image = i.convert("RGB")

            if len(output_images) == 0:
                w = image.size[0]
                h = image.size[1]

            if image.size[0] != w or image.size[1] != h:
                continue

            image = np.array(image).astype(np.float32) / 255.0
            image = torch.from_numpy(image)[None,]
            if 'A' in i.getbands():
                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0
                mask = 1. - torch.from_numpy(mask)
            else:
                try:
                    w = image.size[0]
                    h = image.size[1]
                except:
                    w = 64; h = 64
                mask = torch.zeros(
                    (h, w), dtype=torch.float32, device=device_default)
            output_images.append(image)
            output_masks.append(mask.unsqueeze(0))

        if len(output_images) > 1 and img.format not in excluded_formats:
            output_image = torch.cat(output_images, dim=0)
            output_mask = torch.cat(output_masks, dim=0)
        else:
            output_image = output_images[0]
            output_mask = output_masks[0]

        if invert_mask:
            output_mask = 1.0 - output_mask

        return (output_image, output_mask, image_path)

    def traverse_directory(self, directory):
        for f in os.listdir(directory):
            full_path = os.path.join(directory, f)
            if os.path.isdir(full_path):
                self.traverse_directory(full_path)  # é€’å½’éå†å­æ–‡ä»¶å¤¹
            elif os.path.isfile(full_path):
                ext = os.path.splitext(f)[-1].lower()
                if ext in [e.lower() for e in self.allowed_extensions]:
                    relative_path = os.path.relpath(full_path, self.input_dir)
                    self.files.append(relative_path)

class Save_Image_To_Path:
    """

    """

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.compress_level = 4

    @classmethod
    def INPUT_TYPES(s):
        return {"required":
                {"images": ("IMAGE",),
                 "file_path": ("STRING", {"multiline": True,"default": "","dynamicPrompts": False}),
                 },
                }

    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("IMAGE", "PATH")
    FUNCTION = "save_images"
    OUTPUT_NODE = True

    def save_images(self, images, file_path):
        file_path1 = copy.deepcopy(file_path)
        filename_prefix = os.path.basename(file_path)
        if file_path == '':
            filename_prefix = "ComfyUI"
        filename_prefix, _ = os.path.splitext(filename_prefix)
        _, extension = os.path.splitext(file_path)

        # æ˜¯æ–‡ä»¶åï¼Œéœ€è¦å¤„ç†
        if extension: file_path = os.path.dirname(file_path)

        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(
            filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])

        if not os.path.exists(file_path): os.makedirs(file_path)

        results = list()
        for image in images:
            i = 255. * image.cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))

            # file = f"{filename}_{counter:05}_.png"
            file = f"{filename}.png"

            fp = os.path.join(file_path, file)
            if os.path.exists(fp):
                # file = f"{filename}_{counter:05}_{generate_random_string(8)}.png"
                os.remove(fp)
                fp = os.path.join(file_path, file)
            img.save(os.path.join(file_path, file),
                     compress_level=self.compress_level)
            results.append({
                "filename": file,
                "subfolder": file_path,
                "type": self.type
            })
            counter += 1
        return (images, file_path1)

class Save_Image_Out:
    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.prefix_append = ""
        self.compress_level = 4

    @classmethod
    def INPUT_TYPES(s):
        return {"required":
                {"images": ("IMAGE", ),
                 "filename_prefix": ("STRING", {"default": "ComfyUI"})},
                }

    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("IMAGE", "PATH")
    FUNCTION = "save_images"
    OUTPUT_NODE = True

    def save_images(self, images, filename_prefix="ComfyUI"):
        # images1=copy.deepcopy(images)
        filename_prefix += self.prefix_append
        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(
            filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])
        file_path = full_output_folder
        results = list()
        for image in images:
            i = 255. * image.cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
            file = f"{filename}_{counter:05}_.png"
            fp = os.path.join(file_path, file)
            if os.path.exists(fp):
                # file = f"{filename}_{counter:05}_{generate_random_string(8)}.png"
                os.remove(fp)
                fp = os.path.join(file_path, file)
            img.save(os.path.join(file_path, file),
                     compress_level=self.compress_level)
            results.append({
                "filename": file,
                "subfolder": file_path,
                "type": self.type
            })
            counter += 1
        file = f"{file_path}{filename}_{counter:05}_.png"
        return (images, file,)
        # return { "ui": { "images": results }, "IMAGE":image, "PATH":full_output_folder,}

class image_url_download:
    DESCRIPTION = """å›¾åƒURLä¸‹è½½èŠ‚ç‚¹
    è¾“å…¥è¯´æ˜ï¼š
        image_urls:å›¾åƒURLåˆ—è¡¨ï¼Œæˆ–å•ä¸ªå›¾åƒURLå­—ç¬¦ä¸²
        timeout_single:å•ä¸ªå›¾åƒä¸‹è½½è¶…æ—¶æ—¶é—´(ç§’)
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image_urls": (any,),
                "timeout_single": ("FLOAT", {"default": 10.0, "min": 1.0, "max": 3600.0, "step": 0.001}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "download_images"
    CATEGORY = CATEGORY_NAME

    def download_images(self, image_urls, timeout_single):
        empty_image = torch.zeros((1, 512, 512, 3), dtype=torch.float32)
        """ä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸ºComfyUIæ ¼å¼çš„å¼ é‡"""
        if not image_urls:
            print("Error: image URLs input is none !")
            return (empty_image,)

        if isinstance(image_urls, str):
            image_urls = [image_urls]
        if not isinstance(image_urls, list):
            print("Error: image_urls input is not a list or string !")
            return (empty_image,)
        print(f"ğŸ“¥ Downloading {len(image_urls)} image(s) with {timeout_single}s timeout per image...")

        images = []
        Error_n = 0
        for i, url in tqdm(enumerate(image_urls),):
            try:
                # ä¸‹è½½å›¾åƒï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è¶…æ—¶æ—¶é—´
                response = requests.get(url, timeout=timeout_single)
                response.raise_for_status()
                image_bytes = BytesIO(response.content)
                pil_image = Image.open(image_bytes)
                if pil_image.mode != 'RGB':
                    pil_image = pil_image.convert('RGB')
                image_array = np.array(pil_image, dtype=np.float32) / 255.0
                images.append(image_array)
                
            except Exception as e:
                #print(f"âŒ Error downloading image {i+1}: {e}")
                Error_n += 1
                continue

        if not images:
            print("No images were successfully downloaded")
            # è¿”å›ç©ºçš„å›¾åƒå¼ é‡
            return (empty_image,)

        # æ£€æŸ¥æ‰€æœ‰å›¾åƒçš„å°ºå¯¸ï¼Œå¦‚æœä¸åŒåˆ™è°ƒæ•´ä¸ºç»Ÿä¸€å°ºå¯¸
        if len(images) > 1:
            print(f"âœ… Successfully downloaded {len(images)} image")
            if Error_n > 0:
                print(f"âŒ {Error_n} image(s) failed to download")
            # æ‰¾åˆ°æœ€å¤§çš„é«˜åº¦å’Œå®½åº¦
            max_height = max(img.shape[0] for img in images)
            max_width = max(img.shape[1] for img in images)

            # è°ƒæ•´æ‰€æœ‰å›¾åƒåˆ°ç›¸åŒå°ºå¯¸
            resized_images = []
            for i, img in enumerate(images):
                if img.shape[0] != max_height or img.shape[1] != max_width:
                    # ä½¿ç”¨PILè¿›è¡Œresize
                    pil_img = Image.fromarray((img * 255).astype(np.uint8))
                    pil_img = pil_img.resize((max_width, max_height), Image.Resampling.LANCZOS)
                    resized_img = np.array(pil_img, dtype=np.float32) / 255.0
                    resized_images.append(resized_img)
                    print(f"Resized image {i+1} from {img.shape} to {resized_img.shape}")
                else:
                    resized_images.append(img)
            images = resized_images

        # è½¬æ¢ä¸ºtorchå¼ é‡ï¼Œç»´åº¦é¡ºåºä¸º(æ‰¹æ¬¡ï¼Œé«˜ï¼Œå®½ï¼Œé€šé“)
        images_tensor = torch.stack([torch.from_numpy(img) for img in images])
        #print(f"Final tensor shape: {images_tensor.shape}, dtype: {images_tensor.dtype}")
        #print(f"Batch size: {images_tensor.shape[0]} images")

        return (images_tensor,)


# ------------------image crop nodes------------------
CATEGORY_NAME = "WJNode/ImageCrop"

class adv_crop:
    DESCRIPTION = """
    Advanced Crop Node-20241009
        1. Expand and crop (cropping when negative numbers are input) the image or mask up, down, left, and right (can be processed separately or simultaneously).
        2. Synchronously crop the input mask (can be inconsistent with the image resolution).
        3. Adjustable fill color (black/white, white by default), or fill the original image with reflect, circular, or replicate for edge extension.
        4. Output the background mask after the image is expanded (if the background mask of the output mask is required, no image can be input).

        5. If you want to customize the background color or texture, please use the background mask to blend by yourself.
        6. Method for moving an image: Simultaneously expand and crop in one direction (input negative number + positive number).
        7. Note that cropping should not exceed the boundary (especially when the resolution of the mask is different from that of the image), otherwise an error will be reported.

    é«˜çº§è£å‰ªèŠ‚ç‚¹-20241009
        1:ä¸Šä¸‹å·¦å³æ‰©å±•&è£å‰ª(è¾“å…¥è´Ÿæ•°æ—¶ä¸ºè£å‰ª)å›¾åƒæˆ–é®ç½©(å¯å•ç‹¬æˆ–åŒæ—¶å¤„ç†)
        2:åŒæ­¥è£å‰ªè¾“å…¥é®ç½©(ä¸å›¾åƒåˆ†è¾¨ç‡å¯ä»¥ä¸ä¸€è‡´)
        3:å¯è°ƒæ•´å¡«å……è‰²(é»‘/ç™½ï¼Œé»˜è®¤ç™½è‰²),æˆ–å¡«å……åŸå›¾reflect,circular,æˆ–è¾¹ç¼˜æ‰©å±•replicate
        4:è¾“å‡ºå›¾åƒæ‰©å±•åçš„èƒŒæ™¯é®ç½©(è‹¥éœ€è¾“å‡ºé®ç½©çš„èƒŒæ™¯é®ç½©ï¼Œåˆ™ä¸èƒ½è¾“å…¥å›¾åƒ)

        5:è‡ªå®šä¹‰èƒŒæ™¯/çº¹ç†ï¼šè¯·ä½¿ç”¨èƒŒæ™¯é®ç½©è‡ªè¡Œæ··åˆ
        6:ç§»åŠ¨å›¾ç‰‡çš„æ–¹æ³•ï¼šåœ¨ä¸€ä¸ªæ–¹å‘ä¸ŠåŒæ—¶æ‰©å±•å’Œè£å‰ª(è¾“å…¥è´Ÿæ•°+æ­£æ•°)
        7:æ³¨æ„è£å‰ªä¸è¦è¶…è¿‡è¾¹ç•Œ(ç‰¹åˆ«æ˜¯é®ç½©ä¸å›¾åƒçš„åˆ†è¾¨ç‡ä¸ä¸€æ ·æ—¶)ï¼Œå¦åˆ™ä¼šæŠ¥é”™
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "up": ("INT", {"default": 0, "min": -32768, "max": 32767}),
                "down": ("INT", {"default": 0, "min": -32768, "max": 32767}),
                "left": ("INT", {"default": 0, "min": -32768, "max": 32767}),
                "right": ("INT", {"default": 0, "min": -32768, "max": 32767}),
                "Background": (["White", "Black", "Mirror", "Tile", "Extend"],),
                "InvertValue": ("BOOLEAN", {"default": False}),
                "InvertMask": ("BOOLEAN", {"default": False}),
                "InvertBackMask": ("BOOLEAN", {"default": False})
            },
            "optional": {
                "image": ("IMAGE",),
                "mask": ("MASK",),
            },
        }
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE", "MASK", "MASK")
    RETURN_NAMES = ("image", "mask", "back_mask")
    FUNCTION = "adv_crop"

    def adv_crop(self, up, down, left, right, Background, InvertMask, InvertBackMask, InvertValue, image=None, mask=None):
        Background_mapping = {
            "White": "White",
            "Black": "Black",
            "Mirror": "reflect",
            "Tile": "circular",
            "Extend": "replicate"
        }
        # Map fill method names to function parameters å°†å¡«å……æ–¹å¼åç§°æ˜ å°„åˆ°å‡½æ•°å‚æ•°
        Background = Background_mapping[Background]

        if InvertValue:
            up = -up
            down = -down
            left = -left
            right = -right

        crop_data = np.array([left, right, up, down])
        back_mask = None

        if image is not None:
            image_data = self.get_image_data(image)
            n, h, w, c, *p = image_data
            extend_separate, crop_separate = self.process_crop_data(
                crop_data, h, w)
            image, back_mask = self.data_processing(
                crop_separate, extend_separate, image_data, back_mask, Background)
            if len(list(image.shape)) == 4 and InvertMask:
                image[..., 3] = 1-image[..., 3]

        if mask is not None:  # å•ç‹¬å¤„ç†é®ç½©ä¸å›¾åƒï¼Œå¯åŒæ—¶è£å‰ªä¸åŒåˆ†è¾¨ç‡ä¸åŒæ‰¹æ¬¡çš„é®ç½©ä¸å›¾åƒ
            image_data = self.get_image_data(mask)
            n, h, w, *p = image_data
            extend_separate, crop_separate = self.process_crop_data(
                crop_data, h, w)
            mask, back_mask = self.data_processing(
                crop_separate, extend_separate, image_data, back_mask, Background)
            if InvertMask:
                mask = 1.0 - mask
        if InvertBackMask and back_mask is not None:
            back_mask = 1.0 - back_mask
        return (image, mask, back_mask)

    def data_processing(self, crop_data, extend_data, image_data, back_mask, Background):
        # Obtain image data è·å–å›¾åƒæ•°æ®
        n, h, w, c, dim, image = image_data

        # Expand the image and mask æ‰©å±•èƒŒæ™¯é®ç½©
        back_mask_run = False
        if back_mask is None:
            back_mask_run = True
            back_mask = torch.ones(
                (n, h, w), dtype=torch.float32, device=image.device)
            back_mask = torch.nn.functional.pad(
                back_mask, tuple(extend_data), mode='constant', value=0.0)

        # Expand the image and mask æ‰©å±•å›¾åƒå’ŒèƒŒæ™¯é®ç½©
            # Filling method during expansion æ‰©å±•æ—¶çš„å›¾åƒå¡«å……æ–¹å¼
        fill_color = 1.0
        if Background == "White":
            Background = "constant"
        elif Background == "Black":
            Background = "constant"
            fill_color = 0.0

            # Extended data varies depending on the image or mask
            # æ‰©å±•æ•°æ®å› å›¾åƒæˆ–é®ç½©è€Œå¼‚
        if dim == 4:
            extend_data = tuple(np.concatenate(
                (np.array([0, 0]), extend_data)))
        else:
            extend_data = tuple(extend_data)

            # run Expand the image and mask è¿è¡Œæ‰©å±•å›¾åƒå’ŒèƒŒæ™¯é®ç½©
        if Background == "constant":
            image = torch.nn.functional.pad(
                image, extend_data, mode=Background, value=fill_color)
            print(f"avd_crop:expand image {extend_data} to color")
        else:
            image = torch.nn.functional.pad(
                image, extend_data, mode=Background)
            print(f"avd_crop:expand image {extend_data} to fill")

        # Crop the image and mask è£å‰ªå›¾åƒå’ŒèƒŒæ™¯é®ç½©
        if dim == 4:
            print("avd_crop:crop image")
            n, h, w, c = image.shape
            image = image[:,
                          crop_data[2]:h-crop_data[3],
                          crop_data[0]:w-crop_data[1],
                          :]
        else:
            print("avd_crop:crop mask")
            n, h, w = image.shape
            image = image[:,
                          crop_data[2]:h-crop_data[3],
                          crop_data[0]:w-crop_data[1]
                          ]
        if back_mask_run:
            print("avd_crop:crop back_mask")
            back_mask = back_mask[:,
                                  crop_data[2]:h-crop_data[3],
                                  crop_data[0]:w-crop_data[1]
                                  ]
        return [image, back_mask]

    # Obtaining and standardizing image data è·å–å¹¶æ ‡å‡†åŒ–å›¾åƒæ•°æ®
    def get_image_data(self, image):
        shape = image.shape
        dim = image.dim()
        n, h, w, c = 1, 1, 1, 1
        if dim == 4:
            n, h, w, c = shape
            if c == 1:  # When the last dimension is a single channel, it should be a mask æœ€åä¸€ç»´ä¸ºå•é€šé“æ—¶åº”ä¸ºé®ç½©
                image = image.squeeze(3)
                dim = 3
                print(f"""avd_crop warning: Due to the input not being a standard image tensor,
                      it has been detected that it may be a mask.
                      We are currently converting {shape} to {image.shape} for processing""")
            elif h == 1 and (w != 1 or c != 1):  # When the second dimension is a single channel, it should be a mask ç¬¬2ç»´ä¸ºå•é€šé“æ—¶åº”ä¸ºé®ç½©
                image = image.squeeze(1)
                dim = 3
                print(f"""avd_crop warning: Due to the input not being a standard image/mask tensor,
                      it has been detected that it may be a mask.
                      We are currently converting {shape} to {image.shape} for processing""")
            else:
                print(f"avd_crop:Processing standard images:{shape}")
        elif dim == 3:
            n, h, w = shape
            print(f"avd_crop:Processing standard mask:{shape}")
        elif dim == 5:
            n, c, c1, h, w = shape
            if c == 1 and c1 == 1:  # The mask batch generated by the was plugin may have this issue WASæ’ä»¶ç”Ÿæˆçš„maskæ‰¹æ¬¡å¯èƒ½ä¼šæœ‰æ­¤é—®é¢˜
                image = image.squeeze(1)
                # Remove unnecessary dimensions from mask batch ç§»é™¤maskæ‰¹æ¬¡å¤šä½™çš„ç»´åº¦
                image = image.squeeze(1)
                dim = 3
                print(f"""avd_crop warning: Due to the input not being a standard mask tensor,
                      it has been detected that it may be a mask.
                      We are currently converting {shape} to {image.shape} for processing""")
        else:  # The image dimension is incorrect å›¾åƒç»´åº¦ä¸æ­£ç¡®
            raise ValueError(
                f"avd_crop Error: The shape of the input image or mask data is incorrect, requiring image n, h, w, c mask n, h, w \nWhat was obtained is{shape}")
        return [n, h, w, c, dim, image]

    # Separate cropped data into cropped and expanded data å°†è£å‰ªæ•°æ®åˆ†ç¦»ä¸ºè£å‰ªå’Œæ‰©å±•æ•°æ®
    def process_crop_data(self, crop_data, h, w):
        shape_hw = np.array([h, h, w, w])
        crop_n = crop_data.shape[0]

        # Set the crops_data value that exceeds the boundary on one side to the boundary value of -1
        # å°†å•è¾¹è¶…å‡ºè¾¹ç•Œçš„crop_dataå€¼è®¾ä¸ºè¾¹ç•Œå€¼-1
        for i in range(crop_n):
            if crop_data[i] >= h:
                crop_data[i] = shape_hw[i]-1

        # Determine whether the total height exceeds the boundary åˆ¤æ–­æ€»é«˜åº¦æ˜¯å¦è¶…å‡ºè¾¹ç•Œ
        if crop_data[0]+crop_data[1] >= h:
            raise ValueError(
                f"avd_crop Error:The height {crop_data[0]+crop_data[1]} of the cropped area exceeds the size of image {h}")
        # Determine whether the total width exceeds the boundary åˆ¤æ–­æ€»å®½åº¦æ˜¯å¦è¶…å‡ºè¾¹ç•Œ
        elif crop_data[2]+crop_data[3] >= w:
            raise ValueError(
                f"avd_crop Error:The width {crop_data[2]+crop_data[3]} of the cropped area exceeds the size of image {w}")

        # Separate into cropped and expanded data åˆ†ç¦»ä¸ºè£å‰ªå’Œæ‰©å±•æ•°æ®
        extend_separate = np.array([0, 0, 0, 0])
        corp_separate = np.copy(extend_separate)
        for i in range(crop_n):
            if crop_data[i] < 0:
                extend_separate[i] = abs(crop_data[i])
            else:
                corp_separate[i] = crop_data[i]
        return [extend_separate, corp_separate]

# ------------------image edit nodes------------------
CATEGORY_NAME = "WJNode/ImageEdit"

class invert_channel_adv: #
    DESCRIPTION = """
    Functionality:
    Channel Reorganization: Includes inversion, creation, and replacement of channels, with batch support.
    Channel Operations: Channel separation, channel batching (can be used for calculations that only support masks).
    Batch Matching: If different batch sizes are input, it will attempt to automatically match batches (often fails).

    Input: (Resolution must be the same, batch support is available, try to keep batch sizes consistent)
    RGBA_or_RGB: Input image. If channels are also input, the corresponding channels of this image will be replaced.
    RGBA_Bath: Input RGBA channel batch, used to recombine RGBA channel batches into an image. 
                If this is input, it will ignore the input of RGBA_or_RGB.
    RGB_Bath: Input RGB channel batch, used to recombine RGB channel batches into an image. 
                If RGBA_or_RGB is input, it will replace the RGB channels of the input image.
    R/G/B/A: Input channels (all channels must be of the same size). 
                If only channels are input without an image, an image will be created based on the channels.
    (Replacement Priority: Later channel data will replace earlier ones. 
                R/G/B/A > RGB_Bath > RGBA_Bath > RGBA_or_RGB)

    Output: (All outputs are after replacement, inversion, and other operations)
    RGBA: Output RGBA image, batch support is available. Channels without data will be black.
    RGB: Output RGB image, batch support is available.
    R/G/B/A: Output individual RGBA channels.
    RGB_Bath: Output RGB channel batch, can be used for calculations that only support masks.
    RGBA_Bath: Output RGBA channel batch, can be used for calculations that only support masks.

    åŠŸèƒ½ï¼š
    é€šé“é‡ç»„ï¼šå«åè½¬/æ–°å»º/æ›¿æ¢é€šé“ï¼Œæ”¯æŒæ‰¹æ¬¡
    é€šé“æ“ä½œï¼šåˆ†ç¦»é€šé“ï¼Œè½¬é€šé“æ‰¹æ¬¡(å¯ç”¨äºå°†å›¾ç‰‡è¿›è¡Œä»…æ”¯æŒé®ç½©çš„è®¡ç®—)
    æ‰¹æ¬¡åŒ¹é…ï¼šè‹¥è¾“å…¥äº†ä¸åŒæ‰¹æ¬¡å¤§å°ï¼Œä¼šç®€å•å°è¯•è‡ªåŠ¨åŒ¹é…æ‰¹æ¬¡(å¤šåŠä¼šå¤±è´¥)

    è¾“å…¥ï¼š(åˆ†è¾¨ç‡å¿…é¡»ä¸€æ ·ï¼Œæ”¯æŒæ‰¹æ¬¡ï¼Œæ‰¹æ¬¡å¤§å°å°½é‡ä¸€æ ·)
    RGBA_or_RGBï¼šè¾“å…¥å›¾åƒï¼Œè‹¥åŒæ—¶è¾“å…¥é€šé“ï¼Œåˆ™è¯¥å›¾åƒçš„å¯¹åº”é€šé“ä¼šè¢«æ›¿æ¢ï¼Œ
    RGBA_Bathï¼šè¾“å…¥RGBAé€šé“æ‰¹æ¬¡ï¼Œç”¨äºå°†RGBAé€šé“æ‰¹æ¬¡é‡æ–°ç»„åˆä¸ºå›¾åƒï¼Œ
            è‹¥æ­¤å¤„æœ‰è¾“å…¥åˆ™ä¼šå¿½ç•¥RGBA_or_RGBè¾“å…¥
    RGB_Bathï¼šè¾“å…¥RGBé€šé“æ‰¹æ¬¡ï¼Œç”¨äºå°†RGBé€šé“æ‰¹æ¬¡é‡æ–°ç»„åˆä¸ºå›¾åƒï¼Œ
            è‹¥RGBA_or_RGBæœ‰è¾“å…¥åˆ™æ›¿æ¢è¾“å…¥çš„å›¾åƒrgbé€šé“
    R/G/B/Aï¼šè¾“å…¥é€šé“(æ‰€æœ‰é€šé“é¡»å¤§å°ä¸€æ ·)ï¼Œ
            è‹¥ä»…è¾“å…¥é€šé“ä¸è¾“å…¥å›¾åƒåˆ™æ ¹æ®é€šé“æ–°å»ºå›¾åƒ
    (æ›¿æ¢ä¼˜å…ˆçº§ï¼šåé¢çš„é€šé“æ•°æ®ä¼šè¢«å‰é¢çš„æ›¿æ¢
            R/G/B/A > RGB_Bath > RGBA_Bath > RGBA_or_RGB )

    è¾“å‡ºï¼š(æ‰€æœ‰è¾“å‡ºå‡ä¸ºæ›¿æ¢/åè½¬ç­‰æ“ä½œåçš„)
    RGBAï¼šè¾“å‡ºRGBAå›¾åƒï¼Œæ”¯æŒæ‰¹æ¬¡ï¼Œè‹¥æŸé€šé“æ— æ•°æ®å°†ä¸ºé»‘è‰²
    RGBï¼šè¾“å‡ºRGBå›¾åƒï¼Œæ”¯æŒæ‰¹æ¬¡
    R/G/B/Aï¼šè¾“å‡ºRGBAå•ç‹¬çš„é€šé“
    RGB_Bathï¼šè¾“å‡ºRGBé€šé“æ‰¹æ¬¡ï¼Œå¯ç”¨äºå°†å›¾ç‰‡è¿›è¡Œä»…æ”¯æŒé®ç½©çš„è®¡ç®—
    RGBA_Bathï¼šè¾“å‡ºRGBAé€šé“æ‰¹æ¬¡ï¼Œå¯ç”¨äºå°†å›¾ç‰‡è¿›è¡Œä»…æ”¯æŒé®ç½©çš„è®¡ç®—\
    """
    @classmethod
    def INPUT_TYPES(s):
        device_select = list(device_list.keys())
        device_select[0] = "Auto"
        device_select.insert(0, "Original")
        return {
            "required": {
                "invert_R": ("BOOLEAN", {"default": False}),
                "invert_G": ("BOOLEAN", {"default": False}),
                "invert_B": ("BOOLEAN", {"default": False}),
                "invert_A": ("BOOLEAN", {"default": False}),
                "device": (device_select, {"default": "Original"}),
            },
            "optional": {
                "RGBA_or_RGB": ("IMAGE",),
                "RGBA_Bath": ("MASK",),
                "RGB_Bath": ("MASK",),
                "R": ("MASK",),
                "G": ("MASK",),
                "B": ("MASK",),
                "A": ("MASK",),
            },
        }
    
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE", "IMAGE", "MASK", "MASK", "MASK", "MASK", "MASK", "MASK")
    RETURN_NAMES = ("RGBA", "RGB", "R", "G", "B", "A", "RGB_Bath", "RGBA_Bath")
    FUNCTION = "invert_channel"

    def invert_channel(self, 
                       invert_R, invert_G, invert_B, invert_A, 
                       device, 
                       RGBA_or_RGB=None, RGBA_Bath=None, RGB_Bath=None, 
                       R=None, G=None, B=None, A=None):
        #åˆå§‹åŒ–é€šé“æ•°æ®
        channel_dirt = {"R":None, "G":None, "B":None, "A":None}
        channel_name = list(channel_dirt.keys())

        #è¦æ›¿æ¢çš„é€šé“-RGBA_Bath
        if RGBA_Bath is not None:
            n = RGBA_Bath.shape[0]
            if n % 4 == 0:
                m = int(n / 4)
                for i in range(4):
                    channel_dirt[channel_name[i]] = RGBA_Bath[i*m:(i+1)*m,...]
            else:
                print("Warning: RGBA_Cath mask batch input not RGBA detected, this input will be skipped !")
                print("è­¦å‘Šï¼šæ£€æµ‹åˆ°RGBA_Bathé®ç½©æ‰¹æ¬¡è¾“å…¥ä¸ä¸ºRGBAï¼Œå°†è·³è¿‡æ­¤è¾“å…¥!")

        #è¦æ›¿æ¢çš„é€šé“-RGB_Bath
        if RGB_Bath is not None:
            n = RGB_Bath.shape[0]
            if n % 3 == 0:
                m = int(n / 3)
                for i in range(3):
                    channel_dirt[channel_name[i]] = RGB_Bath[i*m:(i+1)*m,...]
            else:
                print("Warning: RGBA_Cath mask batch input not RGBA detected, this input will be skipped !")
                print("è­¦å‘Šï¼šæ£€æµ‹åˆ°RGB_Bathé®ç½©æ‰¹æ¬¡è¾“å…¥ä¸ä¸ºRGBï¼Œå°†è·³è¿‡æ­¤è¾“å…¥!")

        #è¦æ›¿æ¢çš„å•é€šé“-RGBA
        channel_dirt_temp = {"R":R, "G":G, "B":B, "A":A}
        channel_list_temp = list(channel_dirt_temp.values())
        for i in range(4):
            if channel_list_temp[i] is not None:
                channel_dirt[channel_name[i]] = channel_list_temp[i]

        #è¦æ›¿æ¢æœ€ç»ˆé€šé“
        channel_list = list(channel_dirt.values())
        n_none=channel_list.count(None)
        device_image = None

        # If the input RGBA is not empty, replace the channel 
        # å¦‚æœè¾“å…¥çš„RGBAä¸ä¸ºç©ºï¼Œåˆ™æ›¿æ¢é€šé“
        if RGBA_or_RGB is not None: 
            _, device_image = self.image_device(RGBA_or_RGB, device)# Device selection è®¾å¤‡é€‰æ‹©
            image = RGBA_or_RGB.clone()
            # When the input image has only 3 channels, add an alpha channel 
            # è¾“å…¥å›¾åƒåªæœ‰3é€šé“æ—¶ï¼Œæ·»åŠ ä¸€ä¸ªå…¨1çš„alphaé€šé“
            if image.shape[3] == 3:
                n, h, w, c = image.shape
                image_A = torch.ones((n, h, w, 1), dtype=torch.float32, device=device_image)
                image = torch.cat((image, image_A), dim=-1)

            # å¦‚æœè¾“å…¥çš„RGBAä¸ä¸ºç©ºï¼Œåˆ™æ›¿æ¢é€šé“
            if n_none != 4: 
                for i in range(len(channel_list)):
                    if channel_list[i] is not None:
                        if channel_list[i].shape == image[...,i].shape:
                            image[...,i] = channel_list[i]
                        else:
                            print(f"invert_channel_Warning: The input channel {channel_name [i]} does not match the size of the image. The channel replacement has been skipped!")
                            print(f"invert_channel_è­¦å‘Š(CH): è¾“å…¥çš„é€šé“ {channel_name[i]} ä¸imageå¤§å°ä¸åŒ¹é…,å·²è·³è¿‡è¯¥é€šé“æ›¿æ¢!")

        # If the input RGBA is empty, combine RGBA into an image å¦‚æœè¾“å…¥çš„RGBAä¸ºç©ºï¼Œåˆ™ç»„åˆRGBAä¸ºå›¾åƒ
        else:
            if n_none == 4: # If both the input image and RGBA are empty, an error will be reported å¦‚æœè¾“å…¥imageå’ŒRGBAéƒ½ä¸ºç©ºï¼Œåˆ™æŠ¥é”™
                if RGBA_Bath is not None:
                    raise ValueError(f"invert_channel_Error: Input RGBA_Cath is not an RGBA batch data !\ninvert_channel_é”™è¯¯:è¾“å…¥RGBA_Bathä¸æ˜¯RGBAæ‰¹æ¬¡æ•°æ®ï¼")
                elif RGB_Bath is not None:
                    raise ValueError(f"invert_channel_Error: Input RGB_Cath is not an RGB batch data !\ninvert_channel_é”™è¯¯:è¾“å…¥RGB_Bathä¸æ˜¯RGBæ‰¹æ¬¡æ•°æ®ï¼")
                else:
                    raise ValueError(f"invert_channel_Error: No input image was provided !\ninvert_channel_é”™è¯¯:æœªè¾“å…¥ä»»ä½•å›¾åƒæ•°æ®ï¼")
            # If the image is empty and RGBA is not completely empty, replace the empty channel with all 0s 
            # å¦‚æœimageä¸ºç©º,RGBAä¸å…¨ä¸ºç©ºï¼Œåˆ™å°†ç©ºé€šé“æ›¿æ¢ä¸ºå…¨0
            elif n_none != 0: 
                channel_0 = None
                channel_1 = None
                for i in range(len(channel_list)): # Traverse the channel list and find non empty channels éå†é€šé“åˆ—è¡¨ï¼Œæ‰¾åˆ°ä¸ä¸ºç©ºçš„é€šé“
                    if channel_list[i] is not None:
                        _, device_image = self.image_device(channel_list[i], device)# Device selection è®¾å¤‡é€‰æ‹©
                        channel_0 = torch.zeros(channel_list[i].shape, device=device_image)
                        channel_1 = torch.ones(channel_list[i].shape, device=device_image)
                        break
                # Traverse the channel list and replace empty channels with all 0s or all 1s 
                # éå†é€šé“åˆ—è¡¨ï¼Œå°†ç©ºé€šé“æ›¿æ¢ä¸ºå…¨0æˆ–å…¨1
                for i in range(len(channel_list)): 
                    if channel_list[i] is None:
                        if i != 3:
                            channel_list[i] = channel_0
                        else: # If channel A is empty, replace A with all 1s å¦‚æœAé€šé“ä¸ºç©ºï¼Œåˆ™å°†Aæ›¿æ¢ä¸ºå…¨1
                            channel_list[i] = channel_1
                # Check if the batch quantity is consistent æ£€æµ‹æ‰¹æ¬¡æ•°é‡æ˜¯å¦ä¸€è‡´
                batch_n = [channel_list[i].shape[0] for i in range(len(channel_list))] 
                if max(batch_n) != min(batch_n): 
                    # Repeat the channel with fewer batches to the channel with more batches 
                    # å°†æ‰¹æ¬¡æ•°é‡å°‘çš„é€šé“é‡å¤åˆ°æ‰¹æ¬¡æ•°é‡å¤šçš„é€šé“
                    for i in range(len(channel_list)): 
                        channel_list[i] = channel_list[i].repeat(max(batch_n), 1, 1)
                        print(f"invert_channel_Warning: The input channel batch does not match. The channel {channel_name [i]} batch {image.shape [0]} has been automatically matched to {max (batch_n)}")
                        print(f"invert_channel_è­¦å‘Š(CH): è¾“å…¥çš„é€šé“æ‰¹æ¬¡ä¸åŒ¹é…ï¼Œå·²å°†é€šé“{channel_name[i]}æ‰¹æ¬¡{image.shape[0]}è‡ªåŠ¨åŒ¹é…åˆ°{max(batch_n)}")
            # å°†é€šé“åˆ—è¡¨ä¸­çš„æ¯ä¸ªé€šé“æ·»åŠ ä¸€ä¸ªç»´åº¦ååˆæˆRGBAå›¾åƒ
            try:
                channel_list = [i.unsqueeze(3) for i in channel_list]
                image = torch.cat(channel_list, dim=-1)
            except:
                raise ValueError(f"invert_channel_Error: The input channels do not match the size of the image!")
        
        # Device selection è®¾å¤‡é€‰æ‹©
        _, device_image = self.image_device(image, device)

        # Invert the channel åè½¬é€šé“
        invert = [invert_R, invert_G, invert_B, invert_A]
        if image.shape[3] == 4:
            for i in range(len(invert)):
                if invert[i]:
                    image[..., i] = 1.0 - image[..., i]
        else:
            raise ValueError(
                f"avd_crop Error:The input image should have 3 or 4 dimensions, but got {image.shape}")

        # Separate RGBA images into R, G, B, A å°†RGBAå›¾åƒåˆ†ç¦»ä¸ºR, G, B, A
        image_RGBA = [image[...,i] for i in range(int(image.shape[-1]))]
        image_RGB = image_RGBA[0:int(len(image_RGBA)/4*3)]

        return (image, #RGBA
                image[..., :3], #RGB
                *image_RGBA, #R,G,B,A
                torch.cat(image_RGB, dim=0), #RGB_Bath
                torch.cat(image_RGBA, dim=0) #RGBA_Bath
                )
    
    def image_device(self, image, device):
        # Device selection è®¾å¤‡é€‰æ‹©
        if device == "Auto":
            device_image = device_list["default"]
            image = image.to(device_image)
        elif device == "Original":
            device_image = image.device
        else:
            device_image = device_list[device]
            image = image.to(device_image)
        return [image, device_image]

class ListMerger:
    def __init__(self):
        self.list1_buffer = []
        self.list2_buffer = []
        self.current_index = 0
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "list1": ("*",),
                "list2": ("*",),
                "is_last": ("BOOLEAN", {"default": False}),  # æ ‡è®°æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªå…ƒç´ 
            }
        }
    
    RETURN_TYPES = ("*",)
    RETURN_NAMES = ("merged_list",)
    OUTPUT_IS_LIST = False
    FUNCTION = "merge_lists"
    CATEGORY = CATEGORY_NAME

    def merge_lists(self, list1, list2, is_last):
        # å°†å½“å‰å…ƒç´ æ·»åŠ åˆ°ç¼“å†²åŒº
        self.list1_buffer.append(list1)
        self.list2_buffer.append(list2)
        
        if is_last:
            # æœ€åä¸€ä¸ªå…ƒç´ æ—¶ï¼Œè¿”å›å®Œæ•´çš„åˆå¹¶åˆ—è¡¨
            result = self.list1_buffer + self.list2_buffer
            # æ¸…ç©ºç¼“å†²åŒºï¼Œä¸ºä¸‹ä¸€æ¬¡æ“ä½œåšå‡†å¤‡
            self.list1_buffer = []
            self.list2_buffer = []
            return (result,)
        else:
            # ä¸æ˜¯æœ€åä¸€ä¸ªå…ƒç´ æ—¶ï¼Œè¿”å›Noneæˆ–ç©ºåˆ—è¡¨
            return ([],)

class Bilateral_Filter:
    DESCRIPTION = """
    Image/Mask Bilateral Filtering: Can repair layered distortion caused by color or brightness scaling in images
    CV2 module is required during runtime
    å›¾åƒ/é®ç½©åŒè¾¹æ»¤æ³¢ï¼šå¯ä¿®å¤å›¾åƒå› é¢œè‰²æˆ–äº®åº¦ç¼©æ”¾é€ æˆçš„åˆ†å±‚å¤±çœŸ
    è¿è¡Œæ—¶é¡»cv2æ¨¡å—
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "diameter":("INT",{"default":30,"min":1,"max":2048}),
                "sigma_color":("FLOAT",{"default":75.0,"min":0.01,"max":256.0}),
                "sigma_space":("FLOAT",{"default":75.0,"min":0.01,"max":1024.0}),
            },
            "optional": {
                "image":("IMAGE",),
                "mask":("MASK",),
            }
        }
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE","MASK",)
    FUNCTION = "bilateral"
    def bilateral(self, diameter, sigma_color, sigma_space, image = None, mask = None):
        if image is None:
            if mask is None:
                print("Error: Enter at least one of image and mask !")
            else:
                mask = self.Filter_batch(mask,diameter,sigma_color,sigma_space)
                image = mask.unsqueeze(-1).repeat(1,1,1,3)
        else:
            if mask is None:
                image = self.Filter_batch(image,diameter,sigma_color,sigma_space)
                if image.dim == 4:
                    mask = image[...,-1:]
                else:
                    mask = torch.mean(image, dim=-1, keepdim=False)
            else:
                mask = self.Filter_batch(mask,diameter,sigma_color,sigma_space)
                image = self.Filter_batch(image,diameter,sigma_color,sigma_space)
        return (image, mask)

    def Filter_batch(self,image,diameter,sigma_color,sigma_space):
        # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        shape = image.shape
        if shape[-1] == 3 or len(shape) == 3:
            if shape[0] > 1 :
                image_batch = []
                for i in image:
                    image_batch.append(self.Bilateral_Filter(i,diameter,sigma_color,sigma_space).unsqueeze(0))
                image = torch.cat(image_batch,dim=0)
            else:
                image = self.Bilateral_Filter(image[0],diameter,sigma_color,sigma_space).unsqueeze(0)
        elif shape[-1] == 4 :
            if shape[0] > 1 :
                image_batch = []
                for i in image:
                    image_batch.append(self.Bilateral_Filter_RGBA(i,diameter,sigma_color,sigma_space).unsqueeze(0))
                image = torch.cat(image_batch,dim=0)
            else:
                image = self.Bilateral_Filter_RGBA(image[0],diameter,sigma_color,sigma_space).unsqueeze(0)
        else:
            print("Error: The input is not standard image data, and the original data will be returned !")
        return image
    
    def Bilateral_Filter(self,image,diameter,sigma_color,sigma_space):
        import cv2
        image = np.array(image * 255).astype('uint8')
        image = cv2.bilateralFilter(image, diameter, sigma_color, sigma_space)
        image = torch.tensor(image).float()/255
        return image
    def Bilateral_Filter_RGBA(self,image,diameter,sigma_color,sigma_space):
        import cv2
        image = np.array(image[...,:-1] * 255).astype('uint8')
        image_A = np.array(image[...,-1:].squeeze(-1) * 255).astype('uint8')
        image = cv2.Bilateral_Filter(image, diameter, sigma_color, sigma_space)
        image_A = cv2.Bilateral_Filter(image_A, diameter, sigma_color, sigma_space)
        image = torch.cat((torch.tensor(image),torch.tensor(image_A).unsqueeze(-1)),dim=-1).float()/255
        return image

class image_math:
    DESCRIPTION = """
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "operation":(["-","+","*","&"],{"default":"-"}),
                "algorithm":(["cv2","torch"],{"default":"cv2"}),
                "invert1":("BOOLEAN",{"default":False}),
                "invert2":("BOOLEAN",{"default":False}),
            },
            "optional": {
                "image1":("IMAGE",),
                "image2":("IMAGE",),
                "mask1":("MASK",),
                "mask2":("MASK",),
            }
        }
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE","MASK")
    FUNCTION = "image_math"
    def image_math(self, operation, algorithm, invert1, invert2,
                    mask1=None, mask2=None, image1=None, image2=None):
        mask,image = None,None

        #invert mask
        mask1, image1 = self.invert([mask1, image1],invert1)
        mask2, image2 = self.invert([mask2, image2],invert2)

        #ç»Ÿä¸€æ•°é‡
        mask1, mask2 = self.repeat_mask(mask1, mask2)
        image1, image2 = self.repeat_mask(image1, image2)

        #check cv2
        if algorithm == "cv2":
            try: import cv2
            except:
                print("prompt-mask_and_mask_math: cv2 is not installed, Using Torch")
                print("prompt-mask_and_mask_math: cv2 æœªå®‰è£…, ä½¿ç”¨torch")
                algorithm = "torch"

        #å¦‚æœmaskåªè¾“å…¥ä¸€ä¸ªï¼Œåˆ™è¾“å‡ºè¿™ä¸ª
        if mask1 is None:
            if mask2 is None: pass
            else: mask = mask2
        else :
            if mask2 is None: mask = mask1
            else: mask = self.math(algorithm,operation,mask1,mask2)[0]

        #å¦‚æœimageåªè¾“å…¥ä¸€ä¸ªï¼Œåˆ™è¾“å‡ºè¿™ä¸ª
        if image1 is None:
            if image2 is None: pass
            else: image = image2
        else:
            if image2 is None: image = image1
            else: image = self.math(algorithm,operation,image1,image2)[0]

        return (mask,image)
    
    #æ‰¹é‡ç¿»è½¬
    def invert(self,data,inv):
        if isinstance(data,list):
            data = [1-i if inv and i is not None else i for i in data]
        else:
            if inv and data is not None:
                data = 1-data
        return data

    #æŸä¸ªè¾“å…¥ä¸ºå•å¼ æ—¶ç»Ÿä¸€æ•°é‡
    def repeat_mask(self,mask1,mask2):
        if mask1 is None:
            if mask2 is None: return None,None
            else: return mask2,mask2
        if mask1.shape[0] == 1 and mask2.shape[0] != 1:
            mask1 = mask1.repeat(mask2.shape[0],1,1)
        elif mask1.shape[0] != 1 and mask2.shape[0] == 1:
            mask2 = mask2.repeat(mask1.shape[0],1,1)
        return mask1,mask2

    #æŒ‰æ¨¡å¼è°ƒç”¨è®¡ç®—
    def math(self,algorithm,operation,mask1,mask2):
        #algorithm
        if algorithm == "cv2":
            if operation == "-":
                return (self.subtract_masks(mask1, mask2),)
            elif operation == "+":
                return (self.add_masks(mask1, mask2),)
            elif operation == "*":
                return (self.multiply_masks(mask1, mask2),)
            elif operation == "&":
                return (self.and_masks(mask1, mask2),)
        elif algorithm == "torch":
            if operation == "-":
                return (torch.clamp(mask1 - mask2, min=0, max=1),)
            elif operation == "+":
                return (torch.clamp(mask1 + mask2, min=0, max=1),)
            elif operation == "*":
                return (torch.clamp(mask1 * mask2, min=0, max=1),)
            elif operation == "&":
                mask1 = torch.round(mask1).bool()
                mask2 = torch.round(mask2).bool()
                return (mask1 & mask2, )

    #4ç§è®¡ç®—
    def subtract_masks(self, mask1, mask2):
        mask1 = mask1.cpu()
        mask2 = mask2.cpu()
        cv2_mask1 = np.array(mask1) * 255
        cv2_mask2 = np.array(mask2) * 255
        import cv2
        if cv2_mask1.shape == cv2_mask2.shape:
            cv2_mask = cv2.subtract(cv2_mask1, cv2_mask2)
            return torch.clamp(torch.from_numpy(cv2_mask) / 255.0, min=0, max=1)
        else:
            # do nothing - incompatible mask shape: mostly empty mask
            print("Warning-mask_math: The two masks have different shapes")
            return mask1

    def add_masks(self, mask1, mask2):
        mask1 = mask1.cpu()
        mask2 = mask2.cpu()
        cv2_mask1 = np.array(mask1) * 255
        cv2_mask2 = np.array(mask2) * 255
        import cv2
        if cv2_mask1.shape == cv2_mask2.shape:
            cv2_mask = cv2.add(cv2_mask1, cv2_mask2)
            return torch.clamp(torch.from_numpy(cv2_mask) / 255.0, min=0, max=1)
        else:
            # do nothing - incompatible mask shape: mostly empty mask
            print("Warning-mask_math: The two masks have different shapes")
            return mask1
    
    def multiply_masks(self, mask1, mask2):
        mask1 = mask1.cpu()
        mask2 = mask2.cpu()
        cv2_mask1 = np.array(mask1) * 255
        cv2_mask2 = np.array(mask2) * 255
        import cv2
        if cv2_mask1.shape == cv2_mask2.shape:
            cv2_mask = cv2.multiply(cv2_mask1, cv2_mask2)
            return torch.clamp(torch.from_numpy(cv2_mask) / 255.0, min=0, max=1)
        else:
            # do nothing - incompatible mask shape: mostly empty mask
            print("Warning-mask_math: The two masks have different shapes")
            return mask1
    
    def and_masks(self, mask1, mask2):
        mask1 = mask1.cpu()
        mask2 = mask2.cpu()
        cv2_mask1 = np.array(mask1)
        cv2_mask2 = np.array(mask2)
        import cv2
        if cv2_mask1.shape == cv2_mask2.shape:
            cv2_mask = cv2.bitwise_and(cv2_mask1, cv2_mask2)
            return torch.from_numpy(cv2_mask)
        else:
            # do nothing - incompatible mask shape: mostly empty mask
            print("Warning-mask_math: The two masks have different shapes")
            return mask1

class image_math_value:
    DESCRIPTION = """
    expression: expression
    clamp: If you want to continue with the next image_math_ralue, 
            it is recommended not to open it
    Explanation: The A channel of the image will be automatically removed, 
            and the shape will be the data shape
    Note: This node has been deprecated, please use image_math-value-v1

    expression:è¡¨è¾¾å¼
    clamp:å¦‚æœè¦ç»§ç»­è¿›è¡Œä¸‹ä¸€æ¬¡image_math_valueå»ºè®®ä¸æ‰“å¼€
    è¯´æ˜ï¼šä¼šè‡ªåŠ¨å»æ‰imageçš„Aé€šé“ï¼Œshapeä¸ºæ•°æ®å½¢çŠ¶
    æ³¨æ„ï¼šæ­¤èŠ‚ç‚¹å·²è¢«å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨image_math_value_v1
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "expression":("STRING",{"default":"a+b","multiline": True}),
                "clamp":("BOOLEAN",{"default":True}),
            },
            "optional": {
                "a":("IMAGE",),
                "b":("IMAGE",),
                "c":("MASK",),
                "d":("MASK",),
            }
        }
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE","MASK","LIST")
    RETURN_NAMES = ("image","mask","shape")
    FUNCTION = "image_math"
    def image_math(self,expression,clamp,
                   a=None, b=None, c=None, d=None):
        image = None
        mask = None
        s = 0

        # è·å–æ‰€æœ‰è¾“å…¥çš„å°ºå¯¸å¹¶è®¡ç®—ç›®æ ‡å°ºå¯¸
        shapes = []
        inputs = {'a': a, 'b': b, 'c': c, 'd': d}
        for v in inputs.values():
            if v is not None:
                shapes.append(v.shape[1:3])  # åªå–é«˜åº¦å’Œå®½åº¦
        
        if shapes:
            target_height = min(s[0] for s in shapes)
            target_width = min(s[1] for s in shapes)

            # å¤„ç†æ¯ä¸ªè¾“å…¥
            for k, v in inputs.items():
                if v is not None:
                    # å»æ‰Aé€šé“
                    if k in ['a', 'b'] and v.shape[-1] == 4:
                        v = v[..., 0:-1]
                    
                    # è°ƒæ•´å°ºå¯¸
                    if v.shape[1] != target_height or v.shape[2] != target_width:
                        v = self._resize_tensor(v, target_height, target_width)
                    
                    # é®ç½©è½¬3é€šé“
                    if k in ['c', 'd']:
                        v = v.unsqueeze(-1).expand(-1, -1, -1, 3)
                    
                    inputs[k] = v

        # æ›´æ–°å±€éƒ¨å˜é‡
        a, b, c, d = inputs['a'], inputs['b'], inputs['c'], inputs['d']

        try:
            local_vars = locals().copy()
            exec(f"image = {expression}", {}, local_vars)
            image = local_vars.get("image")
        except:
            print("Warning: Invalid expression !, will output null value.")

        if image is not None:
            if clamp:
                image = torch.clamp(image, 0.0, 1.0)
            s = list(image.shape)
            mask = torch.mean(image, dim=3, keepdim=False)
        return (image,mask,s)

    def _resize_tensor(self, tensor, target_height, target_width):
        """è°ƒæ•´å¼ é‡å°ºå¯¸"""
        if tensor.dim() == 3:  # å¤„ç†mask
            tensor = tensor.unsqueeze(-1)
            tensor = F.interpolate(
                tensor.permute(0, 3, 1, 2),
                size=(target_height, target_width),
                mode='bilinear',
                align_corners=False
            ).permute(0, 2, 3, 1).squeeze(-1)
        else:  # å¤„ç†image
            tensor = F.interpolate(
                tensor.permute(0, 3, 1, 2),
                size=(target_height, target_width),
                mode='bilinear',
                align_corners=False
            ).permute(0, 2, 3, 1)
        return tensor

class Robust_Imager_Merge:
    """å…·æœ‰ä¸€å®šé²æ£’æ€§çš„å›¾åƒå’Œé®ç½©æ‹¼æ¥èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "merge_direction": (["right", "left", "up", "down"], {"default": "right"}),
                "auto_scale": ("BOOLEAN", {"default": True}),
                "output_mode": (["merge", "input1", "input2"], {"default": "merge"}),
            },
            "optional": {
                "image1": ("IMAGE",),
                "image2": ("IMAGE",),
                "mask1": ("MASK",),
                "mask2": ("MASK",),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("image", "mask")
    FUNCTION = "merge_images"
    CATEGORY = CATEGORY_NAME

    def merge_images(self, merge_direction, auto_scale, output_mode,
                    image1=None, image2=None, mask1=None, mask2=None):
        """æ‹¼æ¥å›¾åƒå’Œé®ç½©"""

        # æ£€æŸ¥è¾“å…¥
        inputs = [image1, image2, mask1, mask2]
        input_count = sum(1 for x in inputs if x is not None)

        if input_count == 0:
            raise ValueError("è‡³å°‘éœ€è¦ä¸€ä¸ªè¾“å…¥ï¼ˆimageæˆ–maskï¼‰")

        # å¤„ç†output_modeä¸ºinput1æˆ–input2çš„æƒ…å†µ
        if output_mode == "input1":
            if image1 is not None:
                image_out = image1
                mask_out = self._create_white_mask_like_image(image1)
            elif mask1 is not None:
                image_out = self._mask_to_image(mask1)
                mask_out = mask1
            else:
                raise ValueError("output_modeä¸ºinput1æ—¶ï¼Œå¿…é¡»æœ‰image1æˆ–mask1è¾“å…¥")
            return (image_out, mask_out)

        elif output_mode == "input2":
            if image2 is not None:
                image_out = image2
                mask_out = self._create_white_mask_like_image(image2)
            elif mask2 is not None:
                image_out = self._mask_to_image(mask2)
                mask_out = mask2
            else:
                raise ValueError("output_modeä¸ºinput2æ—¶ï¼Œå¿…é¡»æœ‰image2æˆ–mask2è¾“å…¥")
            return (image_out, mask_out)

        # mergeæ¨¡å¼çš„å¤„ç†é€»è¾‘
        return self._process_merge_mode(merge_direction, auto_scale,
                                      image1, image2, mask1, mask2)

    def _process_merge_mode(self, merge_direction, auto_scale,
                           image1, image2, mask1, mask2):
        """å¤„ç†mergeæ¨¡å¼çš„é€»è¾‘"""

        # ç»Ÿè®¡è¾“å…¥ç±»å‹
        has_image1 = image1 is not None
        has_image2 = image2 is not None
        has_mask1 = mask1 is not None
        has_mask2 = mask2 is not None

        image_count = sum([has_image1, has_image2])
        mask_count = sum([has_mask1, has_mask2])

        # å•ç‹¬è¾“å…¥é€»è¾‘
        if image_count == 1 and mask_count == 0:
            # åªæœ‰ä¸€ä¸ªimageè¾“å…¥
            img = image1 if has_image1 else image2
            return (img, self._create_white_mask_like_image(img))

        elif image_count == 0 and mask_count == 1:
            # åªæœ‰ä¸€ä¸ªmaskè¾“å…¥
            mask = mask1 if has_mask1 else mask2
            return (self._mask_to_image(mask), mask)

        # æ··åˆè¾“å…¥é€»è¾‘
        elif image_count == 1 and mask_count == 1:
            # ä¸€ä¸ªimageå’Œä¸€ä¸ªmask
            img = image1 if has_image1 else image2
            mask = mask1 if has_mask1 else mask2

            # å°†maskè½¬ä¸ºé»‘ç™½å›¾åƒä¸imageæ‹¼æ¥
            mask_as_image = self._mask_to_image(mask)
            merged_image = self._merge_tensors(img, mask_as_image, merge_direction, auto_scale)

            # å°†imageè½¬ä¸ºmaskä¸åŸmaskæ‹¼æ¥
            image_as_mask = self._image_to_mask(img)
            merged_mask = self._merge_tensors(image_as_mask, mask, merge_direction, auto_scale)

            return (merged_image, merged_mask)

        elif image_count == 2 and mask_count == 0:
            # ä¸¤ä¸ªimage
            merged_image = self._merge_tensors(image1, image2, merge_direction, auto_scale)

            # æ£€æŸ¥Alphaé€šé“é€»è¾‘
            has_alpha1 = image1.shape[-1] == 4
            has_alpha2 = image2.shape[-1] == 4

            if has_alpha1 and has_alpha2:
                # éƒ½æœ‰Alphaé€šé“ï¼Œmaskè¾“å‡ºä¸ºæ‹¼æ¥åçš„Alphaé€šé“
                alpha1 = image1[..., 3:4]  # ä¿æŒç»´åº¦
                alpha2 = image2[..., 3:4]
                merged_alpha = self._merge_tensors(alpha1, alpha2, merge_direction, auto_scale)
                merged_mask = merged_alpha.squeeze(-1)  # ç§»é™¤æœ€åä¸€ä¸ªç»´åº¦å˜æˆmaskæ ¼å¼
            else:
                # æ²¡æœ‰Alphaé€šé“æˆ–åªæœ‰ä¸€ä¸ªæœ‰ï¼Œè¾“å‡ºç™½è‰²mask
                merged_mask = self._create_white_mask_like_image(merged_image)

            return (merged_image, merged_mask)

        elif image_count == 0 and mask_count == 2:
            # ä¸¤ä¸ªmask
            merged_mask = self._merge_tensors(mask1, mask2, merge_direction, auto_scale)
            merged_image = self._mask_to_image(merged_mask)
            return (merged_image, merged_mask)

        elif image_count == 2 and mask_count == 1:
            # ä¸¤ä¸ªimageå’Œä¸€ä¸ªmask
            merged_image = self._merge_tensors(image1, image2, merge_direction, auto_scale)
            mask = mask1 if has_mask1 else mask2
            return (merged_image, mask)

        elif image_count == 1 and mask_count == 2:
            # ä¸€ä¸ªimageå’Œä¸¤ä¸ªmask
            img = image1 if has_image1 else image2
            merged_mask = self._merge_tensors(mask1, mask2, merge_direction, auto_scale)
            return (img, merged_mask)

        elif image_count == 2 and mask_count == 2:
            # å…¨éƒ¨è¾“å…¥
            merged_image = self._merge_tensors(image1, image2, merge_direction, auto_scale)
            merged_mask = self._merge_tensors(mask1, mask2, merge_direction, auto_scale)
            return (merged_image, merged_mask)

        else:
            raise ValueError("æœªå¤„ç†çš„è¾“å…¥ç»„åˆ")

    def _create_white_mask_like_image(self, image):
        """åˆ›å»ºä¸å›¾åƒç›¸åŒå¤§å°çš„ç™½è‰²é®ç½©"""
        batch, height, width = image.shape[:3]
        return torch.ones((batch, height, width), dtype=torch.float32)

    def _mask_to_image(self, mask):
        """å°†é®ç½©è½¬æ¢ä¸ºé»‘ç™½å›¾åƒ"""
        # maskæ ¼å¼: (batch, height, width)
        # imageæ ¼å¼: (batch, height, width, channels)
        # åˆ›å»ºRGBå›¾åƒï¼Œæ‰€æœ‰é€šé“éƒ½ä½¿ç”¨maskçš„å€¼
        image = mask.unsqueeze(-1).repeat(1, 1, 1, 3)
        return image

    def _image_to_mask(self, image):
        """å°†å›¾åƒè½¬æ¢ä¸ºé®ç½©ï¼ˆç°åº¦ï¼‰"""
        # imageæ ¼å¼: (batch, height, width, channels)
        # maskæ ¼å¼: (batch, height, width)
        if image.shape[-1] == 1:
            # ç°åº¦å›¾åƒ
            return image.squeeze(-1)
        elif image.shape[-1] >= 3:
            # RGBæˆ–RGBAå›¾åƒï¼Œè½¬æ¢ä¸ºç°åº¦
            # ä½¿ç”¨æ ‡å‡†çš„ç°åº¦è½¬æ¢æƒé‡: 0.299*R + 0.587*G + 0.114*B
            weights = torch.tensor([0.299, 0.587, 0.114], device=image.device, dtype=image.dtype)
            gray = torch.sum(image[..., :3] * weights, dim=-1)
            return gray
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒé€šé“æ•°: {image.shape[-1]}")

    def _merge_tensors(self, tensor1, tensor2, direction, auto_scale):
        """æ‹¼æ¥ä¸¤ä¸ªå¼ é‡"""
        # æ£€æŸ¥æ‰¹æ¬¡ç»´åº¦
        batch1 = tensor1.shape[0]
        batch2 = tensor2.shape[0]

        if batch1 != batch2:
            if batch1 == 1:
                # tensor1æ˜¯å•æ‰¹æ¬¡ï¼Œæ‰©å±•åˆ°ä¸tensor2ç›¸åŒçš„æ‰¹æ¬¡
                tensor1 = tensor1.repeat(batch2, 1, 1, *([1] * (len(tensor1.shape) - 3)))
            elif batch2 == 1:
                # tensor2æ˜¯å•æ‰¹æ¬¡ï¼Œæ‰©å±•åˆ°ä¸tensor1ç›¸åŒçš„æ‰¹æ¬¡
                tensor2 = tensor2.repeat(batch1, 1, 1, *([1] * (len(tensor2.shape) - 3)))
            else:
                raise ValueError(f"æ‰¹æ¬¡æ•°é‡ä¸åŒ¹é…ä¸”éƒ½ä¸ä¸º1: {batch1} vs {batch2}")

        # å¤„ç†Alphaé€šé“
        tensor1, tensor2 = self._handle_alpha_channels(tensor1, tensor2)

        # è‡ªåŠ¨ç¼©æ”¾
        if auto_scale:
            tensor2 = self._auto_scale_tensor(tensor1, tensor2, direction)

        # æ‰§è¡Œæ‹¼æ¥
        if direction == "right":
            return torch.cat([tensor1, tensor2], dim=2)  # æ²¿å®½åº¦æ‹¼æ¥
        elif direction == "left":
            return torch.cat([tensor2, tensor1], dim=2)  # æ²¿å®½åº¦æ‹¼æ¥
        elif direction == "down":
            return torch.cat([tensor1, tensor2], dim=1)  # æ²¿é«˜åº¦æ‹¼æ¥
        elif direction == "up":
            return torch.cat([tensor2, tensor1], dim=1)  # æ²¿é«˜åº¦æ‹¼æ¥
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ‹¼æ¥æ–¹å‘: {direction}")

    def _handle_alpha_channels(self, tensor1, tensor2):
        """å¤„ç†Alphaé€šé“é€»è¾‘"""
        # åªå¯¹å›¾åƒå¼ é‡å¤„ç†Alphaé€šé“ï¼ˆ4ç»´å¼ é‡ï¼‰
        if len(tensor1.shape) != 4 or len(tensor2.shape) != 4:
            return tensor1, tensor2

        channels1 = tensor1.shape[-1]
        channels2 = tensor2.shape[-1]

        # å¦‚æœéƒ½æœ‰Alphaé€šé“ï¼Œä¿æŒä¸å˜
        if channels1 == 4 and channels2 == 4:
            return tensor1, tensor2

        # å¦‚æœéƒ½æ²¡æœ‰Alphaé€šé“ï¼Œä¿æŒä¸å˜
        if channels1 == 3 and channels2 == 3:
            return tensor1, tensor2

        # å¦‚æœä¸€ä¸ªæœ‰Alphaé€šé“ä¸€ä¸ªæ²¡æœ‰ï¼Œå»é™¤Alphaé€šé“
        if channels1 == 4 and channels2 == 3:
            tensor1 = tensor1[..., :3]  # å»é™¤Alphaé€šé“
        elif channels1 == 3 and channels2 == 4:
            tensor2 = tensor2[..., :3]  # å»é™¤Alphaé€šé“

        return tensor1, tensor2

    def _auto_scale_tensor(self, tensor1, tensor2, direction):
        """è‡ªåŠ¨ç¼©æ”¾tensor2ä»¥åŒ¹é…tensor1çš„å¯¹åº”è¾¹ï¼Œä¿æŒåŸæ¯”ä¾‹"""
        current_height = tensor2.shape[1]
        current_width = tensor2.shape[2]

        if direction in ["left", "right"]:
            # å·¦å³æ‹¼æ¥ï¼Œéœ€è¦åŒ¹é…é«˜åº¦ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾å®½åº¦
            target_height = tensor1.shape[1]

            if target_height != current_height:
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                scale_ratio = target_height / current_height
                new_width = int(current_width * scale_ratio)

                # ä½¿ç”¨æ’å€¼ç¼©æ”¾
                if len(tensor2.shape) == 4:  # å›¾åƒ
                    # (batch, height, width, channels) -> (batch, channels, height, width)
                    tensor2_scaled = tensor2.permute(0, 3, 1, 2)
                    tensor2_scaled = torch.nn.functional.interpolate(
                        tensor2_scaled, size=(target_height, new_width),
                        mode='bilinear', align_corners=False
                    )
                    # (batch, channels, height, width) -> (batch, height, width, channels)
                    tensor2 = tensor2_scaled.permute(0, 2, 3, 1)
                else:  # é®ç½©
                    # (batch, height, width) -> (batch, 1, height, width)
                    tensor2_scaled = tensor2.unsqueeze(1)
                    tensor2_scaled = torch.nn.functional.interpolate(
                        tensor2_scaled, size=(target_height, new_width),
                        mode='bilinear', align_corners=False
                    )
                    # (batch, 1, height, width) -> (batch, height, width)
                    tensor2 = tensor2_scaled.squeeze(1)

        elif direction in ["up", "down"]:
            # ä¸Šä¸‹æ‹¼æ¥ï¼Œéœ€è¦åŒ¹é…å®½åº¦ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾é«˜åº¦
            target_width = tensor1.shape[2]

            if target_width != current_width:
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                scale_ratio = target_width / current_width
                new_height = int(current_height * scale_ratio)

                # ä½¿ç”¨æ’å€¼ç¼©æ”¾
                if len(tensor2.shape) == 4:  # å›¾åƒ
                    # (batch, height, width, channels) -> (batch, channels, height, width)
                    tensor2_scaled = tensor2.permute(0, 3, 1, 2)
                    tensor2_scaled = torch.nn.functional.interpolate(
                        tensor2_scaled, size=(new_height, target_width),
                        mode='bilinear', align_corners=False
                    )
                    # (batch, channels, height, width) -> (batch, height, width, channels)
                    tensor2 = tensor2_scaled.permute(0, 2, 3, 1)
                else:  # é®ç½©
                    # (batch, height, width) -> (batch, 1, height, width)
                    tensor2_scaled = tensor2.unsqueeze(1)
                    tensor2_scaled = torch.nn.functional.interpolate(
                        tensor2_scaled, size=(new_height, target_width),
                        mode='bilinear', align_corners=False
                    )
                    # (batch, 1, height, width) -> (batch, height, width)
                    tensor2 = tensor2_scaled.squeeze(1)

        return tensor2

class image_scale_pixel_v2:
    DESCRIPTION = """
    å›¾åƒæŒ‰åƒç´ ç¼©æ”¾
    æ—¨åœ¨è‡ªåŠ¨ç¼©æ”¾å›¾åƒè‡³wanè§†é¢‘æ‰€éœ€å¤§å°ï¼ŒåŠæ§åˆ¶æ€»åƒç´ ä»¥é¿å…oom
    è¾“å…¥è¯´æ˜ï¼š
        TotalPixelsï¼šç¼©æ”¾åçš„æ€»åƒç´ (æ¥è¿‘),å•ä½ï¼šç™¾ä¸‡åƒç´ ï¼Œ
                0.32å¯¹åº”ä¼ª720Pè§†é¢‘ï¼Œ1å¯¹åº”720Pè§†é¢‘ï¼Œ2å¯¹åº”1080Pè§†é¢‘ï¼Œ8.3å¯¹åº”4Kè§†é¢‘
        alignmentï¼šç¼©æ”¾åçš„å®½åº¦å’Œé«˜åº¦å°†ä¸ºalignmentçš„å€æ•°
                wanè§†é¢‘ä¸€èˆ¬æƒ…å†µå¯ä¸º64çš„å€æ•°ï¼Œvace/i2vç­‰éœ€è¦ä¸º128çš„å€æ•°
    è¾“å‡ºè¯´æ˜ï¼š
        é®ç½©å’Œå›¾åƒå„è‡ªç¼©æ”¾ï¼Œå°ºå¯¸äº’ä¸å½±å“
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "TotalPixels": ("FLOAT", {"max": 512.0, "min": 0.0001, "step": 0.0001,"default": "1.0"}),
                "alignment": (["1", "2", "4", "8", "16", "32", "64", "128", "256", "512", "1024"], {"default": "32"}),
            },
            "optional": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "option": ("SO",), #æ–°å¢è¾“å…¥è®¾ç½®
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("image", "mask")
    FUNCTION = "scale_image"
    CATEGORY = CATEGORY_NAME

    def scale_image(self, TotalPixels, alignment, images=None, masks=None, option=None):
        # åˆå§‹åŒ–è®¾ç½®
        option_dict = {"TotalPixels": TotalPixels, 
                       "alignment": alignment, 
                       "size_mode": "crop", 
                       "crop_bbox_method_width": "center",
                       "crop_bbox_method_hight": "center",
                       "method": "LANCZOS",
                       "scale":"original",
                       "fill_color":"#ffffff",
                       "round_width": "round",
                       "round_hight": "round",
                       "Enable_TotalPixels":True,
                       }
        # æ›´æ–°è®¾ç½®
        if option is not None:
            option_dict.update(option)

        # è·å–å‚æ•°
        target_pixels = option_dict["TotalPixels"] * 1000000 # å°†ç™¾ä¸‡åƒç´ è½¬æ¢ä¸ºå®é™…åƒç´ æ•°
        alignment = int(option_dict["alignment"]) # è½¬æ¢alignmentå¯¹é½å€¼ä¸ºæ•´æ•°
        size_mode = option_dict["size_mode"] # ç¼©æ”¾æ—¶çš„å¯¹é½æ–¹å¼
        crop_bbox_method_width = option_dict["crop_bbox_method_width"] # size_modeä¸ºcropæˆ–bboxæ—¶çš„å®½åº¦å¯¹é½çš„åæ ‡åŸºå‡†
        crop_bbox_method_hight = option_dict["crop_bbox_method_hight"] # size_modeä¸ºcropæˆ–bboxçš„é«˜åº¦å¯¹é½çš„åæ ‡åŸºå‡†
        round_width = option_dict["round_width"] # size_modeä¸ºcropæˆ–bboxæ—¶çš„å®½åº¦æ•°å€¼çš„èˆå…¥æ–¹å¼(å››èˆäº”å…¥/å‘ä¸Š/å‘ä¸‹)
        round_hight = option_dict["round_hight"] # size_modeä¸ºcropæˆ–bboxæ—¶çš„é«˜åº¦æ•°å€¼çš„èˆå…¥æ–¹å¼(å››èˆäº”å…¥/å‘ä¸Š/å‘ä¸‹)
        method = option_dict["method"] # å›¾åƒç¼©æ”¾ç®—æ³•
        scale = option_dict["scale"] # å›¾åƒ/é®ç½©å¤§å°çš„æ¯”ä¾‹å­—ç¬¦ä¸²(å®½:é«˜)ï¼Œå¦‚æœä¸ºoriginalåˆ™ä½¿ç”¨è¾“å…¥åŸå§‹å›¾åƒ/é®ç½©çš„å¤§å°
        fill_color = option_dict["fill_color"] # size_modeä¸ºbbox(æ‰©å±•åˆ°å¯¹é½çš„å¤§å°)æ—¶çš„å¡«å……è‰²
        # æ˜¯å¦å¯ç”¨æ€»åƒç´ é™åˆ¶ï¼Œå¦‚æœä¸å¯ç”¨åˆ™æŒ‰åŸå›¾åƒ/é®ç½©è£å‰ªåˆ°æŒ‡å®šæ¯”ä¾‹(å¦‚æœä¸ºoriginalåˆ™ä¸è£å‰ª)åå¯¹é½åˆ°alignmentæŒ‡å®šçš„å‚æ•°(å¦‚æœä¸º1åˆ™ä¸å¤„ç†)
        Enable_TotalPixels = option_dict["Enable_TotalPixels"]

        # å¤„ç†å›¾åƒ
        processed_images = None
        if images is not None:
            processed_images = self._scale_tensor(images, target_pixels, alignment, size_mode, crop_bbox_method_width, crop_bbox_method_hight, round_width, round_hight, method, scale, fill_color, Enable_TotalPixels, is_image=True)
        # å¤„ç†é®ç½©
        processed_masks = None
        if masks is not None:
            processed_masks = self._scale_tensor(masks, target_pixels, alignment, size_mode, crop_bbox_method_width, crop_bbox_method_hight, round_width, round_hight, method, scale, fill_color, Enable_TotalPixels, is_image=False)
        return (processed_images, processed_masks)

    def _scale_tensor(self, tensor, target_pixels, alignment, size_mode, crop_bbox_method_width, crop_bbox_method_hight, round_width, round_hight, method, scale, fill_color, Enable_TotalPixels, is_image=True):
        """ç¼©æ”¾å¼ é‡åˆ°ç›®æ ‡åƒç´ æ•°"""
        import math
        if tensor is None:
            return None
        # è·å–åŸå§‹å°ºå¯¸
        original_height = tensor.shape[1]
        original_width = tensor.shape[2]
        original_pixels = original_height * original_width

        if Enable_TotalPixels:
            # å¯ç”¨æ€»åƒç´ é™åˆ¶ï¼šæŒ‰æ€»åƒç´ æ•°è®¡ç®—ç›®æ ‡å°ºå¯¸
            # 1. æ ¹æ®scaleå‚æ•°ç¡®å®šç›®æ ‡æ¯”ä¾‹
            if scale == "original":
                # ä½¿ç”¨è¾“å…¥å›¾åƒçš„åŸå§‹å®½:é«˜æ¯”ä¾‹
                target_ratio = original_width / original_height
            else:
                # è§£æscaleå‚æ•°çš„æ¯”ä¾‹å­—ç¬¦ä¸²(å®½:é«˜)
                try:
                    width_ratio, height_ratio = map(float, scale.split(':'))
                    target_ratio = width_ratio / height_ratio
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ¯”ä¾‹
                    target_ratio = original_width / original_height

            # 1.1 æ ¹æ®target_pixelså’Œç›®æ ‡æ¯”ä¾‹è®¡ç®—ç›®æ ‡å°ºå¯¸
            # target_pixels = target_width * target_height
            # target_ratio = target_width / target_height
            # æ‰€ä»¥: target_height = sqrt(target_pixels / target_ratio)
            #      target_width = target_height * target_ratio
            target_height = int(math.sqrt(target_pixels / target_ratio))
            target_width = int(target_height * target_ratio)
        else:
            # ä¸å¯ç”¨æ€»åƒç´ é™åˆ¶ï¼šæŒ‰åŸå›¾åƒ/é®ç½©è£å‰ªåˆ°æŒ‡å®šæ¯”ä¾‹åå¯¹é½
            if scale == "original":
                # å¦‚æœä¸ºoriginalåˆ™ä¸è£å‰ªï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å°ºå¯¸
                target_width = original_width
                target_height = original_height
            else:
                # è§£æscaleå‚æ•°çš„æ¯”ä¾‹å­—ç¬¦ä¸²(å®½:é«˜)ï¼ŒæŒ‰æ­¤æ¯”ä¾‹è£å‰ªåŸå›¾åƒ
                try:
                    width_ratio, height_ratio = map(float, scale.split(':'))
                    target_ratio = width_ratio / height_ratio

                    # æŒ‰æ¯”ä¾‹è£å‰ªï¼šé€‰æ‹©è¾ƒå°çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨åŒ…å«
                    if original_width / original_height > target_ratio:
                        # åŸå›¾æ›´å®½ï¼ŒæŒ‰é«˜åº¦è®¡ç®—
                        target_height = original_height
                        target_width = int(original_height * target_ratio)
                    else:
                        # åŸå›¾æ›´é«˜ï¼ŒæŒ‰å®½åº¦è®¡ç®—
                        target_width = original_width
                        target_height = int(original_width / target_ratio)
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å°ºå¯¸
                    target_width = original_width
                    target_height = original_height
        # å¯¹é½åˆ°æŒ‡å®šå€æ•°
        if alignment > 1:
            # æ ¹æ®round_widthé€‰æ‹©å®½åº¦èˆå…¥æ–¹å¼
            if round_width == "ceil":
                target_width = ((target_width + alignment - 1) // alignment) * alignment
            elif round_width == "floor":
                target_width = (target_width // alignment) * alignment
            else:  # round_width == "round"
                target_width = round(target_width / alignment) * alignment

            # æ ¹æ®round_highté€‰æ‹©é«˜åº¦èˆå…¥æ–¹å¼
            if round_hight == "ceil":
                target_height = ((target_height + alignment - 1) // alignment) * alignment
            elif round_hight == "floor":
                target_height = (target_height // alignment) * alignment
            else:  # round_hight == "round"
                target_height = round(target_height / alignment) * alignment
        # ç¡®ä¿æœ€å°å°ºå¯¸
        target_height = max(target_height, alignment)
        target_width = max(target_width, alignment)
        # 2. æ ¹æ®size_modeå¤„ç†
        if size_mode == "fill":
            # æ‹‰ä¼¸æ¨¡å¼ï¼šç›´æ¥æ‹‰ä¼¸åˆ°ç›®æ ‡å°ºå¯¸
            final_height = target_height
            final_width = target_width
            crop_needed = False
            bbox_needed = False
        elif size_mode == "bbox":
            # bboxæ¨¡å¼ï¼šç­‰æ¯”ç¼©æ”¾åå¤–æ‰©å¡«å……åˆ°ç›®æ ‡å°ºå¯¸
            # é€‰æ‹©è¾ƒå°çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å›¾åƒå®Œå…¨åŒ…å«åœ¨ç›®æ ‡å°ºå¯¸å†…
            width_scale = target_width / original_width
            height_scale = target_height / original_height
            scale = min(width_scale, height_scale)

            final_width = int(original_width * scale)
            final_height = int(original_height * scale)
            crop_needed = False
            bbox_needed = True
        else:  # size_mode == "crop"
            # è£å‰ªæ¨¡å¼ï¼šç­‰æ¯”ç¼©æ”¾åè£å‰ª
            crop_needed = True
            bbox_needed = False
            # 3.1: æŒ‰å®½åº¦å¯¹é½åˆ°ç›®æ ‡å®½åº¦ç­‰æ¯”ç¼©æ”¾
            width_scale = target_width / original_width
            scaled_height_by_width = int(original_height * width_scale)
            if scaled_height_by_width > target_height:
                # 3.1: é«˜åº¦è¶…å‡ºï¼ŒæŒ‰å®½åº¦ç¼©æ”¾åè£å‰ªé«˜åº¦
                final_width = target_width
                final_height = scaled_height_by_width
                crop_dim = "height"
                crop_target = target_height
            elif scaled_height_by_width == target_height:
                # å®Œå…¨åŒ¹é…ï¼Œä¸éœ€è¦è£å‰ª
                final_width = target_width
                final_height = target_height
                crop_needed = False
            else:
                # 3.2: é«˜åº¦æ²¡æœ‰è¶…å‡ºï¼ŒæŒ‰é«˜åº¦ç¼©æ”¾åè£å‰ªå®½åº¦
                height_scale = target_height / original_height
                scaled_width_by_height = int(original_width * height_scale)
                final_height = target_height
                final_width = scaled_width_by_height
                crop_dim = "width"
                crop_target = target_width
        # æ‰§è¡Œç¼©æ”¾
        if is_image:
            # å›¾åƒå¼ é‡ (batch, height, width, channels)
            tensor_scaled = tensor.permute(0, 3, 1, 2)  # (batch, channels, height, width)
            # é€‰æ‹©æ’å€¼æ¨¡å¼
            mode_map = {
                "LANCZOS": "bicubic",
                "BILINEAR": "bilinear",
                "BICUBIC": "bicubic",
                "BOX": "nearest",
                "HAMMING": "bilinear",
                "NEAREST": "nearest"
            }
            interpolation_mode = mode_map.get(method, "bilinear")
            tensor_scaled = torch.nn.functional.interpolate(
                tensor_scaled,
                size=(final_height, final_width),
                mode=interpolation_mode,
                align_corners=False if interpolation_mode != "nearest" else None
            )
            # å¦‚æœéœ€è¦è£å‰ªï¼Œæ ¹æ®crop_bbox_methodè¿›è¡Œè£å‰ª
            if crop_needed:
                if crop_dim == "height":
                    # è£å‰ªé«˜åº¦ï¼Œæ ¹æ®crop_bbox_method_hightç¡®å®šèµ·å§‹ä½ç½®
                    if crop_bbox_method_hight == "up":
                        start_h = 0
                    elif crop_bbox_method_hight == "down":
                        start_h = final_height - crop_target
                    else:  # center
                        start_h = (final_height - crop_target) // 2
                    tensor_scaled = tensor_scaled[:, :, start_h:start_h + crop_target, :]
                else:  # crop_dim == "width"
                    # è£å‰ªå®½åº¦ï¼Œæ ¹æ®crop_bbox_method_widthç¡®å®šèµ·å§‹ä½ç½®
                    if crop_bbox_method_width == "left":
                        start_w = 0
                    elif crop_bbox_method_width == "right":
                        start_w = final_width - crop_target
                    else:  # center
                        start_w = (final_width - crop_target) // 2
                    tensor_scaled = tensor_scaled[:, :, :, start_w:start_w + crop_target]

            # å¦‚æœéœ€è¦bboxå¡«å……ï¼Œè¿›è¡Œå¤–æ‰©å¡«å……
            if bbox_needed:
                # è§£æfill_color
                try:
                    fill_color_hex = fill_color.lstrip('#')
                    r = int(fill_color_hex[0:2], 16) / 255.0
                    g = int(fill_color_hex[2:4], 16) / 255.0
                    b = int(fill_color_hex[4:6], 16) / 255.0
                    fill_rgb = [r, g, b]
                except:
                    fill_rgb = [1.0, 1.0, 1.0]  # é»˜è®¤ç™½è‰²

                # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒ
                batch_size = tensor_scaled.shape[0]
                canvas = torch.full((batch_size, 3, target_height, target_width), 0.0, dtype=tensor_scaled.dtype, device=tensor_scaled.device)
                for i in range(3):
                    canvas[:, i, :, :] = fill_rgb[i]

                # è®¡ç®—æ”¾ç½®ä½ç½®
                if crop_bbox_method_width == "left":
                    start_w = 0
                elif crop_bbox_method_width == "right":
                    start_w = target_width - final_width
                else:  # center
                    start_w = (target_width - final_width) // 2

                if crop_bbox_method_hight == "up":
                    start_h = 0
                elif crop_bbox_method_hight == "down":
                    start_h = target_height - final_height
                else:  # center
                    start_h = (target_height - final_height) // 2

                # å°†ç¼©æ”¾åçš„å›¾åƒæ”¾ç½®åˆ°ç”»å¸ƒä¸Š
                canvas[:, :, start_h:start_h + final_height, start_w:start_w + final_width] = tensor_scaled
                tensor_scaled = canvas

            tensor_scaled = tensor_scaled.permute(0, 2, 3, 1)  # (batch, height, width, channels)
        else:
            # é®ç½©å¼ é‡ (batch, height, width)
            tensor_scaled = tensor.unsqueeze(1)  # (batch, 1, height, width)
            tensor_scaled = torch.nn.functional.interpolate(
                tensor_scaled,
                size=(final_height, final_width),
                mode="bilinear",
                align_corners=False
            )
            # å¦‚æœéœ€è¦è£å‰ªï¼Œæ ¹æ®crop_bbox_methodè¿›è¡Œè£å‰ª
            if crop_needed:
                if crop_dim == "height":
                    # è£å‰ªé«˜åº¦ï¼Œæ ¹æ®crop_bbox_method_hightç¡®å®šèµ·å§‹ä½ç½®
                    if crop_bbox_method_hight == "up":
                        start_h = 0
                    elif crop_bbox_method_hight == "down":
                        start_h = final_height - crop_target
                    else:  # center
                        start_h = (final_height - crop_target) // 2
                    tensor_scaled = tensor_scaled[:, :, start_h:start_h + crop_target, :]
                else:  # crop_dim == "width"
                    # è£å‰ªå®½åº¦ï¼Œæ ¹æ®crop_bbox_method_widthç¡®å®šèµ·å§‹ä½ç½®
                    if crop_bbox_method_width == "left":
                        start_w = 0
                    elif crop_bbox_method_width == "right":
                        start_w = final_width - crop_target
                    else:  # center
                        start_w = (final_width - crop_target) // 2
                    tensor_scaled = tensor_scaled[:, :, :, start_w:start_w + crop_target]

            # å¦‚æœéœ€è¦bboxå¡«å……ï¼Œè¿›è¡Œå¤–æ‰©å¡«å……
            if bbox_needed:
                # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œé®ç½©ç”¨0å¡«å……ï¼ˆé»‘è‰²ï¼‰
                batch_size = tensor_scaled.shape[0]
                canvas = torch.zeros((batch_size, 1, target_height, target_width), dtype=tensor_scaled.dtype, device=tensor_scaled.device)

                # è®¡ç®—æ”¾ç½®ä½ç½®
                if crop_bbox_method_width == "left":
                    start_w = 0
                elif crop_bbox_method_width == "right":
                    start_w = target_width - final_width
                else:  # center
                    start_w = (target_width - final_width) // 2

                if crop_bbox_method_hight == "up":
                    start_h = 0
                elif crop_bbox_method_hight == "down":
                    start_h = target_height - final_height
                else:  # center
                    start_h = (target_height - final_height) // 2

                # å°†ç¼©æ”¾åçš„é®ç½©æ”¾ç½®åˆ°ç”»å¸ƒä¸Š
                canvas[:, :, start_h:start_h + final_height, start_w:start_w + final_width] = tensor_scaled
                tensor_scaled = canvas

            tensor_scaled = tensor_scaled.squeeze(1)  # (batch, height, width)
        return tensor_scaled

class image_scale_pixel_option:
    DESCRIPTION = """
    å›¾åƒæŒ‰åƒç´ ç¼©æ”¾çš„é«˜çº§é€‰é¡¹
        é»˜è®¤å‚æ•°å°†å’Œimage_scale_pixel_v2èŠ‚ç‚¹ä¸è¾“å…¥è®¾ç½®æ—¶ä¿æŒä¸€è‡´
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "scale": (["original", "1:1", "3:4", "4:3", "9:16", "16:9"], {"default": "original"}),
                "size_mode": (["fill", "crop","bbox"], {"default": "fill"}),
                "crop_bbox_method_width": (["center", "left", "right"], {"default": "center"}),
                "crop_bbox_method_hight": (["center", "up", "down"], {"default": "center"}),
                "fill_color":("STRING",{"default":"#ffffff"}),
                "round_width": (["round", "ceil", "floor"], {"default": "round"}),
                "round_hight": (["round", "ceil", "floor"], {"default": "round"}),
                "method": (["LANCZOS", "BILINEAR", "BICUBIC", "BOX", "HAMMING", "NEAREST"], {"default":"LANCZOS"}),
                "Enable_TotalPixels":("BOOLEAN",{"default":True}),
            },
            "optional": {
                "refer_to":(any,),
            }
        }

    RETURN_TYPES = ("SO",)
    RETURN_NAMES = ("scale_pixel_option")
    FUNCTION = "scale_image"
    CATEGORY = CATEGORY_NAME

    def scale_image(self, scale, size_mode, crop_bbox_method_width, crop_bbox_method_hight, fill_color, round_width, round_hight, method, Enable_TotalPixels, refer_to=None):
        original_size = scale
        if refer_to is not None :
            # æ£€æµ‹refer_toçš„ç±»å‹
            if isinstance(refer_to, torch.Tensor):
                # æ£€æµ‹åˆ°ä¸ºå›¾åƒç±»
                original_size = self._cast_to_str(refer_to.shape[1:3])
            elif isinstance(refer_to, dict):
                # æ£€æµ‹æ˜¯å¦ä¸ºlatent
                if "samples" in refer_to and isinstance(refer_to["samples"], torch.Tensor):
                    if "type" in refer_to and refer_to["type"] == "audio":
                        print("Warning: The refer_to input is a audio-latent, Cannot obtain aspect ratio, reference input will be ignored")
                    elif refer_to["samples"].dim() == 4:
                        original_size = self._cast_to_str(refer_to["samples"].shape[2:4])
                    else:
                        print("Warning: The refer_to input is unknown samples data , Cannot obtain aspect ratio, reference input will be ignored")
                else:
                    print("Warning1: The refer_to input is not a image/mask/image-latent, please check your input")
            else:
                print("Warning0: The refer_to input is not a image/mask/image-latent, please check your input")
        return ({"scale": original_size,
                 "size_mode": size_mode,
                 "crop_bbox_method_width": crop_bbox_method_width,
                 "crop_bbox_method_hight": crop_bbox_method_hight,
                 "fill_color": fill_color,
                 "round_width": round_width,
                 "round_hight": round_hight,
                 "method": method,
                 "Enable_TotalPixels": Enable_TotalPixels,
                 },)
    
    # è¾“å‡ºçº¦åˆ†åçš„æ¯”ä¾‹
    def _cast_to_str(self, data):
        data = np.array(data)
        h,w = (data/reduce(gcd, data.tolist())).astype(int)
        return str(w)+":"+str(h)


# ------------------Math------------------
CATEGORY_NAME = "WJNode/Math"

class any_math:
    DESCRIPTION = """
        expression: Advanced expression
        Function: Perform numerical calculations on images using expressions.
            The mask will be treated as a 3-channel image.
        Input:
            - a~j: Arbitrary inputs as variables
            - expression1~3: Enter mathematical expressions. The results will be correspondingly output to 1 - 3. Spaces are allowed.
            - PreprocessingTensor: Do you want to preprocess the image 
                (if you need to perform operations on the image, please open it)
                Preprocessing images will unify batches/sizes/channels, 
                allowing image and mask operations to be performed
            - RGBA_to_RGB: When turned on, if the input image is 4-channel, 
                it will be converted to 3-channel.
            - clamp: Limit the image values to 0-1. 
                It is not recommended to turn it on if you want to continue with the next image_math_value.
        Instructions:
            1. Note that if the expression result is not an image (tensor), 
                the result will be output to the corresponding value. 
                In this case, the image and mask outputs will be empty. 
                Conversely, the value will be empty.
            2. Support some Python methods, please pay attention to the output type
            3. Leave the expressions for the outputs you don't need empty to ignore unnecessary calculations. 
            4. ä¸­æ–‡è¯´æ˜è¯·çœ‹ -any_math_v2-
    """
    required = {
                "expression":("STRING",{"default":"a+b","multiline": True}),
                "PreprocessingTensor":("BOOLEAN",{"default":True}),
                "RGBA_to_RGB":("BOOLEAN",{"default":True}),
                "image_clamp":("BOOLEAN",{"default":True}),
                }
    optional = {"a":(any,),"b":(any,),"c":(any,),"d":(any,)}

    def __init__(self):
        self.clamp = True
        self.tensors = {}

    @classmethod
    def INPUT_TYPES(s):
        return {"required":s.required,
                "optional":s.optional}
    CATEGORY = CATEGORY_NAME
    RETURN_TYPES = ("IMAGE","MASK",any)
    RETURN_NAMES = ("image1","mask1","value1")
    FUNCTION = "image_math"

    def image_math(self,expression,PreprocessingTensor,image_clamp,RGBA_to_RGB,**kwargs):
        #åˆå§‹åŒ–å€¼
        self.clamp = image_clamp
        if PreprocessingTensor: 
            self.process_input(RGBA_to_RGB,kwargs)
        else: 
            self.tensors = kwargs
        return (*self.handle_img(expression,1),)

    def process_input(self, RGBA_to_RGB, kwargs):
        try:
            # è·å–æœ‰æ•ˆçš„å¼ é‡è¾“å…¥
            tensor_inputs = {k: v for k, v in kwargs.items() if v is not None and isinstance(v, torch.Tensor)}
            if not tensor_inputs:
                self.tensors = {k: v for k, v in kwargs.items() if v is not None}
                return

            # è·å–å¼ é‡å½¢çŠ¶ä¿¡æ¯å¹¶è®¡ç®—ç›®æ ‡å°ºå¯¸
            shapes = {k: v.shape for k, v in tensor_inputs.items()}
            batch_sizes = []
            for s in shapes.values():
                if len(s) >= 1:  # ç¡®ä¿å¼ é‡è‡³å°‘æœ‰ä¸€ä¸ªç»´åº¦
                    batch_sizes.append(s[0])
                else:
                    batch_sizes.append(1)

            target_batch = max(batch_sizes)
            target_height = min(s[1] for s in shapes.values() if len(s) > 1)
            target_width = min(s[2] for s in shapes.values() if len(s) > 2)
            target_channels = 3 if (max((s[-1] for s in shapes.values() if len(s) >= 3), default=3) != 4 or RGBA_to_RGB) else 4

            # æ£€æŸ¥æ‰¹æ¬¡å¤§å°
            multi_batches = {b for b in batch_sizes if b > 1}
            if len(multi_batches) > 1:
                print(f"Warning: Multiple different batch sizes detected {multi_batches} May affect the calculation results!")

            # å¤„ç†æ‰€æœ‰è¾“å…¥
            for k, v in kwargs.items():
                if v is None:
                    continue

                if isinstance(v, torch.Tensor):
                    # å¤„ç†ç»´åº¦
                    if v.dim() == 2:
                        v = v.unsqueeze(0)
                    elif v.dim() == 3 and v.shape[-1] != target_channels:
                        v = v.unsqueeze(-1)

                    # æ‰¹æ¬¡å¤„ç†
                    if v.shape[0] < target_batch:
                        repeat_times = target_batch // v.shape[0]
                        if repeat_times * v.shape[0] == target_batch:
                            v = v.repeat(repeat_times, *(1 for _ in range(v.dim()-1)))
                        else:
                            v = v.expand(target_batch, *(v.shape[i] for i in range(1, v.dim())))

                    # å°ºå¯¸è°ƒæ•´
                    if v.dim() >= 3 and (v.shape[1] != target_height or v.shape[2] != target_width):
                        if v.dim() == 3:
                            v = v.unsqueeze(-1)
                            v = self._resize_tensor(v, target_height, target_width)
                            v = v.squeeze(-1)
                        else:
                            v = self._resize_tensor(v, target_height, target_width)

                    # é€šé“å¤„ç†
                    if v.dim() == 3:
                        v = v.unsqueeze(-1).expand(-1, -1, -1, target_channels)
                    elif v.dim() == 4:
                        if v.shape[-1] == 4 and RGBA_to_RGB:
                            v = v[..., :target_channels]
                        elif v.shape[-1] == 1:
                            v = v.expand(-1, -1, -1, target_channels)

                    self.tensors[k] = v
                else:
                    self.tensors[k] = v

        except Exception as e:
            print(f"Error details:{str(e)}")
            raise ValueError("Preprocessing failed! Please check the image input, such as whether the quantity is consistent(any_math)")

    def _resize_tensor(self, tensor, target_height, target_width):
        """è°ƒæ•´å¼ é‡å°ºå¯¸"""
        return F.interpolate(
            tensor.permute(0, 3, 1, 2),
            size=(target_height, target_width),
            mode='bilinear',
            align_corners=False
        ).permute(0, 2, 3, 1)

    def _process_channels(self, tensor, target_channels, RGBA_to_RGB):
        """å¤„ç†å¼ é‡é€šé“"""
        if tensor.dim() == 3:
            return tensor.unsqueeze(-1).expand(-1, -1, -1, target_channels)
        elif tensor.dim() == 4:
            return tensor[..., :target_channels] if (tensor.shape[-1] == 4 and RGBA_to_RGB) else tensor
        else:
            print(f"è­¦å‘Šï¼šè¾“å…¥å¼ é‡ç»´åº¦å¼‚å¸¸ï¼(ç»´åº¦ï¼š{tensor.dim()})")
            return tensor

    def handle_img(self,expression,n=1):
        #åˆå§‹åŒ–è¾“å‡º
        mask,image,data = None,None,None
        #æ£€æŸ¥å­—ç¬¦ä¸²
        try:
            if expression not in [""," ",None]:
                safe, reason = is_safe_eval(expression)
                if safe: image = eval(expression, {}, self.tensors)
                else: print("warn: "+ reason + ",Ignore this expression"+ str(n))
        except:
            print(f"Error: Expression{n} error! (image_math_value_v2)")

        #è‹¥ç»“æœä¸ä¸ºç©ºåˆ™è¿›è¡Œå¤„ç†
        if image is not None:
            try:
                if isinstance(image,torch.Tensor): #è‹¥è¾“å‡ºä¸ºå¼ é‡(å›¾åƒç±»)
                    if self.clamp:#æ˜¯å¦é’³åˆ¶åˆ°0-1
                        image = torch.clamp(image, 0.0, 1.0) 
                    if image.dim() == 3: #å¦‚æœç»“æœæ˜¯é®ç½©ï¼Œå°†é®ç½©ä½œä¸ºé€šé“è½¬ä¸ºå›¾åƒè¾“å‡º
                        mask = image
                        image = mask.unsqueeze(-1).expand(-1, -1, -1, 3)
                        print("Warning: The result may be a mask. (image_math_value_v2)")
                    elif image.dim() == 4: #å¦‚æœç»“æœæ˜¯å›¾åƒï¼Œå°†å›¾åƒä»¥æ±‚å‡å€¼çš„æ–¹å¼å‹ç¼©ä¸ºé®ç½©
                        mask = torch.mean(image, dim=3, keepdim=False)
                else:
                    data = image #è‹¥è¾“å‡ºä¸ºéå¼ é‡ç›´æ¥è¿”å›
            except:
                print("Warning: You have calculated non image data! (image_math_value_v2)")
        return (image,mask,data)

class any_math_v2(any_math):
    DESCRIPTION = """
        expression:é«˜çº§è¡¨è¾¾å¼
            åŠŸèƒ½ï¼šä½¿ç”¨è¡¨è¾¾å¼å¯¹å›¾åƒè¿›è¡Œæ•°å€¼è®¡ç®—ï¼Œmaskå°†è¢«è§†ä¸º3é€šé“å›¾åƒ
        è¾“å…¥ï¼š
            a~jï¼šè¾“å…¥ä»»æ„ä½œä¸ºå˜é‡
            expressionï¼šè¾“å…¥æ•°å­¦è¡¨è¾¾å¼ï¼Œå¯åŒ…å«ç©ºæ ¼
            PreprocessingTensor:æ˜¯å¦é¢„å¤„ç†å›¾åƒ(è‹¥éœ€è¦å¯¹å›¾åƒè¿›è¡Œè¿ç®—è¯·æ‰“å¼€)
                é¢„å¤„ç†å›¾åƒä¼šå°†æ‰¹æ¬¡/å¤§å°/é€šé“ç»Ÿä¸€ï¼Œä½¿imageå’Œmaskå¯ä»¥è¿›è¡Œè¿ç®—
            RGBA_to_RGBï¼šæ‰“å¼€æ—¶å¦‚æœè¾“å…¥imageä¸º4é€šé“åˆ™å°†å…¶è½¬ä¸º3é€šé“
            clamp:é™åˆ¶å›¾åƒæ•°å€¼ä¸º0-1ï¼Œå¦‚æœè¦ç»§ç»­è¿›è¡Œä¸‹ä¸€æ¬¡image_math_valueå»ºè®®ä¸æ‰“å¼€
        è¯´æ˜ï¼š
            1ï¼šæ³¨æ„è¡¨è¾¾å¼ç»“æœè‹¥ä¸æ˜¯å›¾åƒ(å¼ é‡)ï¼Œåˆ™ç»“æœå°†è¾“å‡ºåˆ°å¯¹åº”çš„value
                æ­¤æ—¶imageå’Œmaskè¾“å‡ºå°†ä¸ºç©ºï¼Œåä¹‹valueä¸ºç©º
            2ï¼šæ”¯æŒéƒ¨åˆ†pythonæ–¹æ³•ï¼Œè¯·æ³¨æ„è¾“å‡ºç±»å‹
            3ï¼šä¸éœ€è¦çš„è¾“å‡ºå¯¹åº”çš„è¡¨è¾¾å¼è¯·ç•™ç©ºï¼Œå¯å¿½ç•¥ä¸å¿…è¦çš„è®¡ç®—
            4ï¼šPlease refer to the English explanation -any_math-
    """

    @classmethod
    def INPUT_TYPES(s):
        required = {"expression1":("STRING",{"default":"a+b","multiline": True}),
                    "expression2":("STRING",{"default":"","multiline": True}),
                    "expression3":("STRING",{"default":"","multiline": True}),
                    **s.required}
        required.pop("expression")
        return {
                "required":required,
                "optional":{**s.optional,
                    "e":(any,),"f":(any,),"g":(any,),"h":(any,),"i":(any,),"j":(any,)}
                }

    RETURN_TYPES = ("IMAGE","MASK",any,"IMAGE","MASK",any,"IMAGE","MASK",any)
    RETURN_NAMES = ("image1","mask1","value1","image2","mask2","value2","image3","mask3","value3")
    def image_math(self,expression1,expression2,expression,PreprocessingTensor,image_clamp,RGBA_to_RGB,**kwargs):
        #åˆå§‹åŒ–å€¼
        self.clamp = image_clamp
        if PreprocessingTensor: 
            self.process_input(RGBA_to_RGB,kwargs)
        else: 
            self.tensors = kwargs

        return (*self.handle_img(expression1,1),
                *self.handle_img(expression2,2),
                *self.handle_img(expression,3),
                )

# The following is a test and has not been imported yet ä»¥ä¸‹ä¸ºæµ‹è¯•ï¼Œæš‚æœªå¯¼å…¥
CATEGORY_NAME = "WJNode/temp"

class SaveImage1: #å‚è€ƒ
    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.prefix_append = ""
        self.compress_level = 4

    @classmethod
    def INPUT_TYPES(s):
        return {"required":
                {"images": ("IMAGE", ),
                 "filename_prefix": ("STRING", {"default": "ComfyUI"})},
                "hidden": {"prompt": "PROMPT", "extra_pnginfo": "EXTRA_PNGINFO"},
                }

    CATEGORY = "WJNode/Image"
    RETURN_TYPES = ("IMAGE", )
    # RETURN_NAMES = ("IMAGE", )
    FUNCTION = "save_images"
    OUTPUT_NODE = True

    def save_images(self, images, filename_prefix="ComfyUI", prompt=None, extra_pnginfo=None):
        filename_prefix += self.prefix_append
        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(
            filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])
        results = list()
        for (batch_number, image) in enumerate(images):
            i = 255. * image.cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))

            filename_with_batch_num = filename.replace(
                "%batch_num%", str(batch_number))
            file = f"{filename_with_batch_num}_{counter:05}_.png"
            img.save(os.path.join(full_output_folder, file),
                     compress_level=self.compress_level)
            results.append({
                "filename": file,
                "subfolder": subfolder,
                "type": self.type
            })
            counter += 1
        return {"ui": {"images": results}, "IMAGE": images}


NODE_CLASS_MAPPINGS = {
    #WJNode/ImageFile
    "Load_Image_From_Path": Load_Image_From_Path,
    "Save_Image_To_Path": Save_Image_To_Path,
    "Save_Image_Out": Save_Image_Out,
    "Load_Image_Adv": Load_Image_Adv,
    "image_url_download": image_url_download,
    #WJNode/ImageCrop
    "adv_crop": adv_crop,
    #WJNode/ImageEdit
    "invert_channel_adv": invert_channel_adv,
    "ListMerger": ListMerger,
    "Bilateral_Filter": Bilateral_Filter,
    "image_math": image_math,
    "image_math_value": image_math_value,
    "Robust_Imager_Merge": Robust_Imager_Merge,
    "image_scale_pixel_v2": image_scale_pixel_v2,
    "image_scale_pixel_option": image_scale_pixel_option,
    #WJNode/ImageMath
    "any_math": any_math,
    "any_math_v2": any_math_v2,
}
